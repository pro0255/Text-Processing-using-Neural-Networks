{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def adding_module_path():\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "adding_module_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preparing.build.gutenberg_builder import GutenbergBuilder\n",
    "from src.app.project_setup import ProjectSetup\n",
    "from src.data_preparing.split.run_split_deps_on_stats import run_split_deps_on_stats_same_dir\n",
    "from src.utils.create_path_to_gutenberg import create_path_to_gutenberg_sentence_authors_sentence, create_path_to_gutenberg_authors\n",
    "from src.utils.create_test_dataset_from import create_test_dataset_from\n",
    "from src.data_preparing.split.run_split_deps_on_stats import run_split_deps_on_stats_same_dir\n",
    "from src.utils.create_path_to_gutenberg import create_path_to_gutenberg_sentence_authors_sentence\n",
    "from src.config.config import PATH_TO_DATASET_FOLDER_TEST, PATH_TO_DATASET_FOLDER_TEST, AUTHORS_FILE_NAME\n",
    "\n",
    "from src.tokenizers.prepare_dataset_from_tokenizer import prepare_dataset_from_tokenizer\n",
    "from src.tokenizers.transformer_tokenizer import TransformerTokenizer\n",
    "import tensorflow as tf\n",
    "from src.encoder.create_encoder_from_path import create_encoder_from_path\n",
    "from src.testing.get_testing_dataset import dataset as test_dataset\n",
    "from src.data_loading.get_dataset_object_from import get_dataset_object_from_path, get_datasets\n",
    "from src.utils.create_path_to_gutenberg import get_paths_to_gutenberg, get_path_to_gutenberg_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data_test'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_DATASET_FOLDER_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running preperation of packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vojta\\Desktop\\diploma\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vojta\\Desktop\\diploma\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Vojta\\Desktop\\diploma\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "from src.experiments.settings import LearningSettings\n",
    "from transformers import AutoConfig\n",
    "from src.types.transformer_name import TransformerName\n",
    "from src.types.processing_type import PreprocessingType\n",
    "from src.preprocessing.preprocessing_factory import PreprocessingFactory\n",
    "from src.types.transformer_pooling import TransformerPooling\n",
    "from src.config.run_prep import run_prep\n",
    "run_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = LearningSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = TransformerName.BertBaseUncased.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shorting method with min = 3\n",
      "Creating lemma method with instance <WordNetLemmatizer>\n"
     ]
    }
   ],
   "source": [
    "preprocessing_factory = PreprocessingFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data, path_authors = get_path_to_gutenberg_sets(6, 3, PATH_TO_DATASET_FOLDER_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/usp/pro0255/diploma/data_test/gutenberg/10Authors/Sentence3/train.csv',\n",
       " '/home/usp/pro0255/diploma/data_test/gutenberg/10Authors/Sentence3/valid.csv',\n",
       " '/home/usp/pro0255/diploma/data_test/gutenberg/10Authors/Sentence3/test.csv')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_datasets(path_data, ';', preprocessing_factory.create(PreprocessingType.Default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.experiment_run_wrapper import ExperimentRunWrapper\n",
    "from src.experiments.settings import LearningSettings\n",
    "from src.tokenizers.transformer_tokenizer import TransformerTokenizer\n",
    "from src.encoder.create_encoder_from_path import create_encoder_from_path\n",
    "from transformers import TFAutoModel, AutoConfig\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from src.tokenizers.prepare_dataset_from_tokenizer import prepare_dataset_from_tokenizer\n",
    "from src.models.transformer.bert_pooling_layer import BertPoolingLayer\n",
    "import time\n",
    "from src.experiments.experiment_summarization import ExperimentSummarization\n",
    "from src.types.experiment_summarization_fields import ExperimentSummarizationFields\n",
    "from src.config.learning_config import TRANSFORMER_EPOCHS\n",
    "from src.experiments.descriptions.create_description import (\n",
    "    create_description_for_transformer_with_dense_head\n",
    ")\n",
    "import os\n",
    "from src.config.config import TEXT_COLUMN, LABEL_COLUMN, VALIDATION_SIZE\n",
    "from src.experiments.experiment_description import ExperimentDescription\n",
    "from src.types.transformer_pooling import TransformerPooling\n",
    "from src.types.prediction_model_type import PredictionModelType\n",
    "from src.types.net_type import NetType\n",
    "from src.types.embedding_type import EmbeddingType\n",
    "from src.types.processing_type import PreprocessingType\n",
    "import time\n",
    "from src.data_loading.get_dataset_object_from import get_dataset_all\n",
    "from src.utils.from_dataset_arrays import from_dataset_dataframe\n",
    "from src.utils.split_dataframe import split_dataframe\n",
    "from src.utils.normalize_dataframe_to_size import normalize_dataframe_to_size\n",
    "from src.utils.create_dataset_from_dataframe import create_dataset_from_Xy\n",
    "import pandas as pd\n",
    "from src.utils.generate_random_stamp import generator_random_stamp\n",
    "from src.types.transformer_pooling import TransformerPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loading.get_dataset_object_from import get_dataset_all\n",
    "NUMBER_OF_AUTHORS = 6\n",
    "NUMBER_OF_SENTENCES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating shorting method with min = 3\n",
      "Creating lemma method with instance <WordNetLemmatizer>\n",
      "Loading dataset from=C:\\Users\\Vojta\\Desktop\\diploma\\data\\gutenberg\\6Authors\\Sentence3\\data.csv\n"
     ]
    }
   ],
   "source": [
    "data, paths = get_dataset_all(NUMBER_OF_AUTHORS, NUMBER_OF_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = from_dataset_dataframe(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize_dataframe_to_size(all_data, 500)\n",
    "X_train, X_test, y_train, y_test = split_dataframe(data_normalized)\n",
    "df = pd.DataFrame()\n",
    "df[TEXT_COLUMN] = X_train\n",
    "df[LABEL_COLUMN] = y_train\n",
    "X_train, X_val, y_train, y_val = split_dataframe(df, VALIDATION_SIZE)\n",
    "\n",
    "train_ds = create_dataset_from_Xy(X_train, y_train)\n",
    "test_ds = create_dataset_from_Xy(X_test, y_test)\n",
    "val_ds = create_dataset_from_Xy(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13195    guess guess fun like need afraid end possible ...\n",
       "16684    coming wanted moved saw overcome ike trim bran...\n",
       "38535    father asked nefert doubtfully friend listen u...\n",
       "28576    possibly disloyalty parent sign satanic source...\n",
       "37822    foundation temple outside port cocquerel immed...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'guess guess fun like need afraid end possible guessing furnish meaning word sure right word hint form sound spelling throw hint keep secret slightest slight shadow hint lie meagerly suggestive fact spalleggiato carry word egg stomach'>, <tf.Tensor: shape=(), dtype=int32, numpy=53>)\n"
     ]
    }
   ],
   "source": [
    "for x in val_ds:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'thank goodness come open door s missis screech'>, <tf.Tensor: shape=(), dtype=int32, numpy=1865>)\n"
     ]
    }
   ],
   "source": [
    "for x in data[0]:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TransformerTokenizer(\n",
    "    model_name, \n",
    "    create_encoder_from_path(\n",
    "        paths[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enum import Enum\n",
    "\n",
    "class TransformerPooling(Enum):\n",
    "    LastHiddenState = \"last_hidden_state\"\n",
    "    Pooler = \"pooler_output\"\n",
    "    HiddenStates = \"hidden_states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TFAutoModel.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPoolingLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, pooling_type):\n",
    "        if pooling_type == TransformerPooling.LastHiddenState:\n",
    "            last_hidden_state = inputs[TransformerPooling.LastHiddenState.value]\n",
    "            return tf.reduce_mean(last_hidden_state, axis=1)\n",
    "            \n",
    "            \n",
    "        if pooling_type == TransformerPooling.Pooler:\n",
    "            pooler = inputs[TransformerPooling.Pooler.value]\n",
    "            return pooler\n",
    "        \n",
    "        \n",
    "        # if pooling_type == TransformerPooling.HiddenStates:\n",
    "        #     #TODO: deal with hidden state\n",
    "        #     selector = inputs[TransformerPooling.HiddenStates.value]\n",
    "            \n",
    "        #     layers = tf.convert_to_tensor(selector)[-1]\n",
    "            \n",
    "        #     if tf.shape(tf.shape(layers)) > 3:\n",
    "        #         #print(layers)\n",
    "        #         print('A')\n",
    "        #         layers_together = tf.reduce_mean(layers, axis=0)\n",
    "        #         average_sentence_words = tf.reduce_mean(layers_together, axis=1)\n",
    "        #         cls = layers_together[:, 0, :]\n",
    "        #         return cls\n",
    "                \n",
    "                \n",
    "        #         #print(average_sentence_words)\n",
    "        #         #layers = tf.reduce_mean(layers, axis=0, keepdims=False)\n",
    "        #         #cls = layers[:, 0, :]\n",
    "        #         #average_sentence_words = tf.reduce_mean(layers[:, 1:tf.shape(layers)[1]-1, :], axis=1)\n",
    "        #         #return average_sentence_words\n",
    "        #     else:\n",
    "        #         print('B')\n",
    "                \n",
    "        #         #return tf.reduce_mean(layers, axis=1)\n",
    "        #         cls = layers[:, 0, :]\n",
    "        #         #return cls\n",
    "        #         average_sentence_words = tf.reduce_mean(layers[:, 1:tf.shape(layers)[1]-1, :], axis=1)\n",
    "        #         return average_sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from src.types.transformer_pooling import TransformerPooling\n",
    "from src.types.transformer_pooling_strategy import TransformerPoolingStrategy\n",
    "\n",
    "\n",
    "def verify_bert_pooling_input(\n",
    "    pooling_type,\n",
    "    transformer_pooling_strategy=None,\n",
    "    transformer_start_index=None,\n",
    "    transformer_end_index=None,\n",
    "):\n",
    "    if pooling_type in [\n",
    "        TransformerPooling.LastHiddenState,\n",
    "        TransformerPooling.Pooler,\n",
    "    ] and (\n",
    "        transformer_pooling_strategy is not None\n",
    "        or transformer_start_index != -1\n",
    "        or transformer_end_index != -1\n",
    "    ):\n",
    "        assert Exception(\n",
    "            f\"Cannot use pooling strategy when is not used {TransformerPooling.HiddenStates.value}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "class BertPoolingLayer(tf.keras.layers.Layer):\n",
    "    def call(\n",
    "        self,\n",
    "        inputs,\n",
    "        pooling_type,\n",
    "        transformer_pooling_strategy=None,\n",
    "        transformer_start_index=-1,\n",
    "        transformer_end_index=-1,\n",
    "    ):\n",
    "        verify_bert_pooling_input(\n",
    "            pooling_type,\n",
    "            transformer_pooling_strategy,\n",
    "            transformer_start_index,\n",
    "            transformer_end_index,\n",
    "        )\n",
    "\n",
    "        if pooling_type == TransformerPooling.LastHiddenState:\n",
    "            last_hidden_state = inputs[TransformerPooling.LastHiddenState.value]\n",
    "            return tf.reduce_mean(last_hidden_state, axis=1)\n",
    "\n",
    "        if pooling_type == TransformerPooling.Pooler:\n",
    "            pooler = inputs[TransformerPooling.Pooler.value]\n",
    "            return pooler\n",
    "\n",
    "        if pooling_type == TransformerPooling.HiddenStates:\n",
    "            selector = inputs[TransformerPooling.HiddenStates.value]\n",
    "\n",
    "            number_of_layers = len(selector) - 1\n",
    "            index_start_from_behinde = number_of_layers - transformer_start_index\n",
    "            index_end_from_behinde = number_of_layers - transformer_end_index + 1\n",
    "\n",
    "            selector = selector[index_start_from_behinde:index_end_from_behinde]\n",
    "\n",
    "            if transformer_pooling_strategy in [\n",
    "                TransformerPoolingStrategy.ConcatAverage,\n",
    "                TransformerPoolingStrategy.ConcatCLS,\n",
    "            ]:\n",
    "                concatened = tf.concat(selector, axis=2)\n",
    "\n",
    "                if transformer_pooling_strategy == TransformerPoolingStrategy.ConcatCLS:\n",
    "                    cls = concatened[:, 0, :]\n",
    "                    return cls\n",
    "                else:\n",
    "                    averaged_sentence = tf.reduce_mean(concatened, axis=1)\n",
    "                    return averaged_sentence\n",
    "            else:\n",
    "                tf_tensor = tf.convert_to_tensor(selector)\n",
    "                averaged = tf.reduce_mean(tf_tensor, axis=0)\n",
    "                if transformer_pooling_strategy == TransformerPoolingStrategy.CLS:\n",
    "                    cls = averaged[:, 0, :]\n",
    "                    return cls\n",
    "                else:\n",
    "                    averaged_sentence = tf.reduce_mean(averaged, axis=1)\n",
    "                    return averaged_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.instances.neural_nets.transformer_with_dense_head import TransformerWithDenseHeadExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = TransformerWithDenseHeadExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = exp.get_model(False, model_name, 10, 512, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(512, ), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(512, ), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = transformer(input_ids, attention_mask=mask)  # we only keep tensor 0 (last_hidden_state)\n",
    "X = BertPoolingLayer()(embeddings, TransformerPooling.Pooler)\n",
    "#X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "y = tf.keras.layers.Dense(10, activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=(                                               \n",
      "                                (None, 512, 768),                                                 \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768),                                                \n",
      "                                 (None, 512, 768)),                                               \n",
      "                                 attentions=None, c                                               \n",
      "                                ross_attentions=Non                                               \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " bert_pooling_layer_10 (BertPoo  (None, 768)         0           ['tf_bert_model_2[0][0]',        \n",
      " lingLayer)                                                       'tf_bert_model_2[0][1]',        \n",
      "                                                                  'tf_bert_model_2[0][2]',        \n",
      "                                                                  'tf_bert_model_2[0][3]',        \n",
      "                                                                  'tf_bert_model_2[0][4]',        \n",
      "                                                                  'tf_bert_model_2[0][5]',        \n",
      "                                                                  'tf_bert_model_2[0][6]',        \n",
      "                                                                  'tf_bert_model_2[0][7]',        \n",
      "                                                                  'tf_bert_model_2[0][8]',        \n",
      "                                                                  'tf_bert_model_2[0][9]',        \n",
      "                                                                  'tf_bert_model_2[0][10]',       \n",
      "                                                                  'tf_bert_model_2[0][11]',       \n",
      "                                                                  'tf_bert_model_2[0][12]',       \n",
      "                                                                  'tf_bert_model_2[0][13]',       \n",
      "                                                                  'tf_bert_model_2[0][14]']       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 768)         3072        ['bert_pooling_layer_10[0][0]']  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          98432       ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_117 (Dropout)          (None, 128)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 10)           1290        ['dropout_117[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,585,034\n",
      "Trainable params: 101,258\n",
      "Non-trainable params: 109,483,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=settings.loss, optimizer=settings.optimizer, metrics=settings.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[0].take(1000)\n",
    "val = data[0].take(1000)\n",
    "test = data[0].take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  2/113 [..............................] - ETA: 1:59:30 - loss: 3.2204 - accuracy: 0.1094"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\Models.ipynb Cell 33'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model1\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=1'>2</a>\u001b[0m    prepare_dataset_from_tokenizer(train_ds, tokenizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49mshuffle(buffer_size\u001b[39m=\u001b[39;49m\u001b[39m30000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=3'>4</a>\u001b[0m     \u001b[39m.\u001b[39;49mcache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39;49mbatch(\u001b[39m32\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39;49mprefetch(\u001b[39m4\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=6'>7</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mprepare_dataset_from_tokenizer(val_ds, tokenizer)\u001b[39m.\u001b[39;49mbatch(\u001b[39m1\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=7'>8</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/Models.ipynb#ch0000019?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1208'>1209</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1209'>1210</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1210'>1211</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1211'>1212</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1212'>1213</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1213'>1214</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1214'>1215</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1215'>1216</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1216'>1217</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/engine/training.py?line=1217'>1218</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=906'>907</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=908'>909</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=909'>910</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=912'>913</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=938'>939</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=939'>940</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=940'>941</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=941'>942</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=942'>943</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=3126'>3127</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=3127'>3128</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=3128'>3129</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=3129'>3130</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=3130'>3131</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1954'>1955</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1955'>1956</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1956'>1957</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1957'>1958</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1958'>1959</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1959'>1960</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1960'>1961</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1961'>1962</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1962'>1963</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1963'>1964</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1964'>1965</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=595'>596</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=596'>597</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=597'>598</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=598'>599</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=599'>600</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=600'>601</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=601'>602</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=602'>603</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=603'>604</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=604'>605</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=605'>606</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=606'>607</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=609'>610</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/function.py?line=610'>611</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=57'>58</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=58'>59</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=59'>60</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=60'>61</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model1.fit(\n",
    "   prepare_dataset_from_tokenizer(train_ds, tokenizer)\n",
    "    .shuffle(buffer_size=30000)\n",
    "    .cache()\n",
    "    .batch(32)\n",
    "    .prefetch(4),\n",
    "    validation_data=prepare_dataset_from_tokenizer(val_ds, tokenizer).batch(1),\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(prepare_dataset_from_tokenizer(test, tokenizer).take(10).batch(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 8, 0, 0, 0, 0, 8, 0])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "for x in prepare_dataset_from_tokenizer(train, tokenizer).batch(10):\n",
    "    text, label = x\n",
    "    output = transformer(text, output_hidden_states=True)\n",
    "    output = BertPoolingLayer()(output, TransformerPooling.HiddenStates)\n",
    "    #print(output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [372]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m prepare_dataset_from_tokenizer(\u001b[43mdataset\u001b[49m, tokenizer)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for x in prepare_dataset_from_tokenizer(dataset, tokenizer).batch(10):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model experiment with to lower\n",
    "\n",
    "... find best preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ideas\n",
    "\n",
    "- one dataset - combination of preprocessing\n",
    "- size of input (k) - deps on bert - make statistics\n",
    "- type of transformer - distilbert, larbe bert etc\n",
    "- head of transformer - \n",
    "- encoding of transformer\n",
    "- classic model of tranformer\n",
    "- testovn proti Glove LSTM, Embedding Dense, Glove LSTM, Glove Dense\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "deac0217989ddcfa4345bc236e90ab22a38c5ad7d8517b867015082eaa3f672d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
