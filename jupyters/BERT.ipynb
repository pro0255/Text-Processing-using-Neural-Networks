{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "773f905d-14ef-4c4c-bdb8-c189a61e548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET_FOLDER = \"C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6e0393d8-4aae-4f7b-a9f0-74186a8b9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c5fca69e-d98d-4410-8de9-65e150241de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Enum):\n",
    "    Gutenberg = \"gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3455e0a4-827d-4f43-a8e3-6f3d19ceee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetType(Enum):\n",
    "    Sentence = \"Sentence\"\n",
    "    Article = \"Article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d786e504-e51a-44b7-aaa2-07a85dafde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e1afd2ff-9f61-484b-843f-5324ca49a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(directory, dataset, dataset_type, k=None):\n",
    "    is_sentence_type = dataset_type == DataSetType.Sentence \n",
    "    if is_sentence_type and k is None:\n",
    "        raise Exception(f\"Sentence should be specified with k argument!\")\n",
    "    \n",
    "    return os.path.join(directory, dataset.value, dataset_type.value + str(k), DATA_NAME) if is_sentence_type else os.path.join(directory, dataset.value, dataset_type.value, DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "30989200-3082-4386-9437-12c3f74395b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vojta\\\\Desktop\\\\diploma\\\\data\\\\gutenberg\\\\Sentence10\\\\data.csv'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d42fe3c2-01ef-40a3-99d1-34ec57a91233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "655da5bb-54e4-4fef-9a11-54f2c8c6601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_object_from_path(csv_filename, delim, text_pipeline_func=None):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_filename)\n",
    "    \n",
    "    def parse_csv(line):\n",
    "        csv_line = bytes.decode(line.numpy())\n",
    "        text, author = csv_line.split(delim)\n",
    "        if text_pipeline_func is not None:\n",
    "            text = text_pipeline_func(text)\n",
    "        return text, author \n",
    "\n",
    "    dataset = dataset.map(lambda tpl: tf.py_function(parse_csv, [tpl], [tf.string, tf.string]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "50f92cc0-bba3-4050-a08b-b4d4f9d37cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset_object_from_path(create_path(PATH_TO_DATASET_FOLDER, DataSet.Gutenberg, DataSetType.Sentence, 3), ';', process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e356074-6cc7-48b4-8b50-6f34e1810f36",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550384b-b8b7-4e3d-8e40-68cef96e842f",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/tree/master/examples/tensorflow/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2767ab0f-b59b-412b-8657-181a23ca7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d641a92b-cc9b-4d27-a06c-d17d4f216969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6de27c8d-1a80-4922-805a-e85280ebf24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"the tragedy of pudd'nhead wilson  by mark twain     a whisper to the reader       _there is no character, howsoever good and fine, but it can      be destroyed by ridicule, howsoever poor and witless. observe the ass, for instance: his character is about      perfect, he is the choicest spirit among all the humbler      animals, yet see what ridicule has brought him to. instead      of feeling complimented when we are called an ass, we are      left in doubt._ --pudd'nhead wilson's calendar  a person who is ignorant of legal matters is always liable to make mistakes when he tries to photograph a court scene with his pen and so i was not willing to let the law chapters in this book go to press without first subjecting them to rigid and exhausting revision and correction by a trained barrister--if that is what they are called.\">, <tf.Tensor: shape=(), dtype=string, numpy=b'Twain, Mark'>)\n"
     ]
    }
   ],
   "source": [
    "for line in ds:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5f2b4c72-f0b2-4229-bc3b-f55cf24ce3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset imdb (C:\\Users\\Vojta\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12839ab59794404094d42ddb1c90e70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2ab15842-91ee-47e6-81ff-ebf60fc5831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62819f-6738-46c3-9514-c1d3d73e293f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b857a4fa-0733-4744-9e31-7b77d4bb1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9fe9268a-d291-48d5-930f-58507bde95c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e35ab776334a3ca086b3e4761d4bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea9015aff734b8eb1e7f0659f20bc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710ccb5728034075b12ed3deae834c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "13bba6a5-53bd-47b8-8d28-d1fc98c07579",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_imdb[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_imdb[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "\n",
    "\n",
    "full_train_dataset = tokenized_imdb[\"train\"]\n",
    "full_eval_dataset = tokenized_imdb[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f98cddb3-10b8-43ea-999a-81a15cab9563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...', 'label': 1, 'input_ids': [101, 1247, 1110, 1185, 6796, 1120, 1155, 1206, 3144, 2852, 1105, 26890, 1197, 1133, 1103, 1864, 1115, 1241, 1132, 2021, 1326, 1164, 5973, 6969, 119, 26890, 1197, 2736, 19501, 1183, 117, 3144, 2852, 2736, 5263, 119, 26890, 1197, 15836, 1132, 2385, 3014, 119, 3144, 2852, 112, 188, 4928, 1132, 1677, 1167, 8277, 119, 119, 119, 3144, 2852, 2736, 1167, 1176, 3460, 15463, 20629, 117, 1191, 1195, 1138, 1106, 3205, 12672, 119, 119, 119, 1109, 1514, 1959, 1110, 4780, 1105, 6994, 1186, 117, 1133, 1138, 107, 172, 20293, 12716, 3923, 107, 119, 2563, 1176, 1106, 14133, 117, 1106, 3942, 117, 1106, 17459, 119, 1731, 1164, 1198, 8965, 136, 16819, 1645, 1315, 117, 1234, 2269, 3144, 2852, 2736, 1237, 1133, 117, 1113, 1103, 1168, 1289, 117, 8995, 1152, 9353, 1237, 1326, 113, 106, 106, 106, 114, 119, 2389, 1122, 112, 188, 1103, 1846, 117, 1137, 1103, 4840, 117, 1133, 146, 1341, 1142, 1326, 1110, 1167, 1483, 1190, 1237, 119, 1650, 1103, 1236, 117, 1103, 5681, 1132, 1541, 1363, 1105, 6276, 119, 1109, 3176, 1110, 1136, 26558, 1120, 1155, 119, 119, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "for x in small_train_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "22f36193-044f-4edc-a613-88006cbe13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "\n",
    "\n",
    "tf_train_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "tf_eval_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d0fff663-20fe-4d2e-b72a-0d2e3457c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'input_ids': <tf.Tensor: shape=(512,), dtype=int64, numpy=\n",
      "array([  101,  1247,  1110,  1185,  6796,  1120,  1155,  1206,  3144,\n",
      "        2852,  1105, 26890,  1197,  1133,  1103,  1864,  1115,  1241,\n",
      "        1132,  2021,  1326,  1164,  5973,  6969,   119, 26890,  1197,\n",
      "        2736, 19501,  1183,   117,  3144,  2852,  2736,  5263,   119,\n",
      "       26890,  1197, 15836,  1132,  2385,  3014,   119,  3144,  2852,\n",
      "         112,   188,  4928,  1132,  1677,  1167,  8277,   119,   119,\n",
      "         119,  3144,  2852,  2736,  1167,  1176,  3460, 15463, 20629,\n",
      "         117,  1191,  1195,  1138,  1106,  3205, 12672,   119,   119,\n",
      "         119,  1109,  1514,  1959,  1110,  4780,  1105,  6994,  1186,\n",
      "         117,  1133,  1138,   107,   172, 20293, 12716,  3923,   107,\n",
      "         119,  2563,  1176,  1106, 14133,   117,  1106,  3942,   117,\n",
      "        1106, 17459,   119,  1731,  1164,  1198,  8965,   136, 16819,\n",
      "        1645,  1315,   117,  1234,  2269,  3144,  2852,  2736,  1237,\n",
      "        1133,   117,  1113,  1103,  1168,  1289,   117,  8995,  1152,\n",
      "        9353,  1237,  1326,   113,   106,   106,   106,   114,   119,\n",
      "        2389,  1122,   112,   188,  1103,  1846,   117,  1137,  1103,\n",
      "        4840,   117,  1133,   146,  1341,  1142,  1326,  1110,  1167,\n",
      "        1483,  1190,  1237,   119,  1650,  1103,  1236,   117,  1103,\n",
      "        5681,  1132,  1541,  1363,  1105,  6276,   119,  1109,  3176,\n",
      "        1110,  1136, 26558,  1120,  1155,   119,   119,   119,   102,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0],\n",
      "      dtype=int64)>, 'token_type_ids': <tf.Tensor: shape=(512,), dtype=int64, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int64)>}\n"
     ]
    }
   ],
   "source": [
    "for x in tf_train_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "71b6cbb9-713d-410c-9752-7c21d2bc3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {x: tf.cast(tf_train_dataset[x], dtype=tf.int32) for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
    "\n",
    "\n",
    "# train_tf_dataset = tf_train_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
    "\n",
    "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
    "# eval_tf_dataset = eval_tf_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e9d6d61c-8831-4b66-8e09-e1094568f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
      "array([  101,  1247,  1110,  1185,  6796,  1120,  1155,  1206,  3144,\n",
      "        2852,  1105, 26890,  1197,  1133,  1103,  1864,  1115,  1241,\n",
      "        1132,  2021,  1326,  1164,  5973,  6969,   119, 26890,  1197,\n",
      "        2736, 19501,  1183,   117,  3144,  2852,  2736,  5263,   119,\n",
      "       26890,  1197, 15836,  1132,  2385,  3014,   119,  3144,  2852,\n",
      "         112,   188,  4928,  1132,  1677,  1167,  8277,   119,   119,\n",
      "         119,  3144,  2852,  2736,  1167,  1176,  3460, 15463, 20629,\n",
      "         117,  1191,  1195,  1138,  1106,  3205, 12672,   119,   119,\n",
      "         119,  1109,  1514,  1959,  1110,  4780,  1105,  6994,  1186,\n",
      "         117,  1133,  1138,   107,   172, 20293, 12716,  3923,   107,\n",
      "         119,  2563,  1176,  1106, 14133,   117,  1106,  3942,   117,\n",
      "        1106, 17459,   119,  1731,  1164,  1198,  8965,   136, 16819,\n",
      "        1645,  1315,   117,  1234,  2269,  3144,  2852,  2736,  1237,\n",
      "        1133,   117,  1113,  1103,  1168,  1289,   117,  8995,  1152,\n",
      "        9353,  1237,  1326,   113,   106,   106,   106,   114,   119,\n",
      "        2389,  1122,   112,   188,  1103,  1846,   117,  1137,  1103,\n",
      "        4840,   117,  1133,   146,  1341,  1142,  1326,  1110,  1167,\n",
      "        1483,  1190,  1237,   119,  1650,  1103,  1236,   117,  1103,\n",
      "        5681,  1132,  1541,  1363,  1105,  6276,   119,  1109,  3176,\n",
      "        1110,  1136, 26558,  1120,  1155,   119,   119,   119,   102,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0])>, 'token_type_ids': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
      "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0])>, 'attention_mask': <tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0])>}, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for x in train_tf_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "53ec387b-7249-45e4-acfb-f0ed4d2e9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d0155d32-9117-433f-b768-dd90dbc2a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_795 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3317b539-c255-41ba-9807-fa91f957848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# model.fit(train_tf_dataset.batch(8), validation_data=eval_tf_dataset, epochs=2, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "130f7485-af97-40a4-ba16-c69766a7b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(train_tf_dataset.take(100).batch(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651f17e-4076-4c66-92d7-a74e55a1d1b4",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/06/why-and-how-to-use-bert-for-nlp-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd365327-1031-48c8-a0bd-b10ea8139785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86909045-e0f7-4690-8b68-c08495f58d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset.\n",
      "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='C:\\\\Users\\\\Vojta\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', \n",
    "          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "          as_supervised=True,\n",
    "          with_info=True)\n",
    "print('info', ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3710f97a-4e9b-4c59-983e-cc97221d4cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review This was an absolutely terrible movie. Don't be lu 0\n",
      "review I have been known to fall asleep during films, but 0\n",
      "review Mann photographs the Alberta Rocky Mountains in a  0\n",
      "review This is the kind of film for a snowy Sunday aftern 1\n",
      "review As others have mentioned, all the women that go nu 1\n"
     ]
    }
   ],
   "source": [
    "for review, label in tfds.as_numpy(ds_train.take(5)):\n",
    "    print('review', review.decode()[0:50], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f2f4c46-f7ce-4fd4-8ae3-a9f8e73e8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91a20d6-9895-425c-a2b3-d099e74c73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "    return tokenizer(review, truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4daef4b-13a1-47e7-911a-9fe34e483bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c6afe0-0d2a-430d-b51d-7ddc7630dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to the expected input to TFBertForSequenceClassification, see here \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "    }, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "\n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = convert_example_to_feature(review.decode())\n",
    "        \n",
    "        \n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01940cf7-369d-4bed-93d5-6078e1d3de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 47s\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train dataset\n",
    "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(ds_test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99a22766-7476-4c43-a884-d098d9f5b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccb617da-f5fb-4aa7-9841-7ee759ea4602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d11e53e66348e984e8493442fa4e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7b1e5555242c8a71b8e4594af89d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 1\n",
    "\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5047a2-cd9d-47f4-95b2-45b3c2ed14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "bert_history = model.fit(ds_train_encoded.take(100), epochs=number_of_epochs, validation_data=ds_test_encoded.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049cdd0-70e8-47a1-b803-1361012588e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e22bdea-9a2e-4ed4-a492-2bf78910b453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c221fa-1a24-4b2b-a3ef-d2045870c063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b3d9c2-0136-4185-88bf-58ce466b3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7592, 1010, 1026, 7308, 1028, 3899, 2003, 10140, 102]\n",
      "{'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, my dog is cute\"\n",
    "\n",
    "encoded = tokenizer.encode(\"Hello, <MASK> dog is cute\")\n",
    "\n",
    "encoded1 = tokenizer(\"Hello, my dog is cute\", truncation=True, padding='max_length', max_length=512)\n",
    "print(encoded)\n",
    "print(encoded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b6e0ee0-71cf-45ff-83aa-269b744f081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[b'input_ids' b'token_type_ids' b'attention_mask']], shape=(1, 3), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.constant(encoded)[None, :]  # Batch size 1\n",
    "\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a03c085-a97c-47a1-968f-98c90b2e7e67",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mencoded1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094895c7-9994-4e83-8b08-f31297f75e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='input_ids')\n",
    "input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='attention_mask')\n",
    "input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='token_type_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bf70488-f553-4092-a692-2726da5d9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 8, 768), dtype=float32, numpy=\n",
      "array([[[-0.11437128,  0.19371368,  0.1249586 , ..., -0.38269076,\n",
      "          0.21065912,  0.54070807],\n",
      "        [ 0.53082436,  0.32074896,  0.3664591 , ..., -0.00360684,\n",
      "          0.7578602 ,  0.0388434 ],\n",
      "        [-0.48765117,  0.88492435,  0.42556354, ..., -0.697621  ,\n",
      "          0.44583344,  0.12309441],\n",
      "        ...,\n",
      "        [-0.70027906, -0.18150637,  0.32969648, ..., -0.48379344,\n",
      "          0.06802348,  0.890084  ],\n",
      "        [-1.0354631 , -0.2566779 , -0.03165286, ...,  0.31974417,\n",
      "          0.39990202,  0.17954747],\n",
      "        [ 0.6079918 ,  0.2609707 , -0.31307253, ...,  0.03109779,\n",
      "         -0.6282723 , -0.19942449]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
      "array([[-7.19458342e-01, -2.14449644e-01, -2.95759737e-01,\n",
      "         3.66029590e-01,  2.79677331e-01,  2.21835729e-02,\n",
      "         5.72991431e-01,  6.23311587e-02,  5.95868900e-02,\n",
      "        -9.99652624e-01,  5.01465015e-02,  4.47561204e-01,\n",
      "         9.76118922e-01,  3.39878760e-02,  8.44945073e-01,\n",
      "        -3.69052559e-01,  9.86494571e-02, -3.71687144e-01,\n",
      "         1.73706472e-01,  1.15147747e-01,  4.41331208e-01,\n",
      "         9.95248556e-01,  3.72208655e-01,  8.28809440e-02,\n",
      "         2.14019760e-01,  6.89648628e-01, -6.10419333e-01,\n",
      "         8.71356666e-01,  9.41583991e-01,  5.73720753e-01,\n",
      "        -3.21870297e-01,  8.66708625e-03, -9.86105084e-01,\n",
      "        -2.05422062e-02, -4.37554538e-01, -9.80115473e-01,\n",
      "         1.11424424e-01, -6.75871849e-01,  1.34990081e-01,\n",
      "         3.11300367e-01, -8.29972148e-01,  1.90055773e-01,\n",
      "         9.98961151e-01, -3.17983150e-01,  2.15169713e-02,\n",
      "        -1.65308073e-01, -9.99433219e-01,  1.01733029e-01,\n",
      "        -8.18114161e-01,  3.31183448e-02,  3.67399961e-01,\n",
      "        -7.32311830e-02, -1.42609045e-01,  1.89070687e-01,\n",
      "         2.61186361e-01,  4.15824383e-01, -2.44265869e-01,\n",
      "        -5.98463789e-02, -7.34919235e-02, -3.42015684e-01,\n",
      "        -5.80011606e-01,  2.83311933e-01, -5.05132079e-01,\n",
      "        -8.19669366e-01,  1.98132247e-01,  1.91079676e-01,\n",
      "         3.70112024e-02, -1.13269426e-01,  1.34724483e-01,\n",
      "        -2.16143519e-01,  6.34941339e-01,  2.48689149e-02,\n",
      "         3.82873207e-01, -8.17786157e-01, -2.48737127e-01,\n",
      "         8.49820450e-02, -5.29981613e-01,  9.99997675e-01,\n",
      "        -5.21543138e-02, -9.70523298e-01,  3.98477256e-01,\n",
      "         2.13589948e-02,  3.90350431e-01,  3.55884939e-01,\n",
      "        -1.78814620e-01, -9.99974489e-01,  2.69391149e-01,\n",
      "        -3.80567797e-02, -9.86571848e-01,  6.93217590e-02,\n",
      "         3.91383469e-01, -2.18842607e-02, -9.63317081e-02,\n",
      "         3.85454774e-01, -3.41361314e-01, -8.03617463e-02,\n",
      "        -3.20222378e-02, -3.63279104e-01, -7.81294554e-02,\n",
      "         1.91923864e-02, -1.34291798e-01, -1.60132926e-02,\n",
      "        -5.26398309e-02, -2.80061662e-01,  9.36112404e-02,\n",
      "        -2.28847370e-01, -1.23054549e-01, -1.10021353e-01,\n",
      "        -3.28081638e-01,  4.03563440e-01,  2.80478925e-01,\n",
      "        -2.01019332e-01,  2.76847303e-01, -9.40230846e-01,\n",
      "         4.17562783e-01, -1.54725432e-01, -9.75533187e-01,\n",
      "        -4.30033803e-01, -9.85457897e-01,  5.91576397e-01,\n",
      "         3.73436660e-02, -1.93200365e-01,  9.16911066e-01,\n",
      "         3.60116005e-01,  1.45048201e-01,  1.53982162e-01,\n",
      "        -1.06565002e-02, -9.99999523e-01, -3.15730065e-01,\n",
      "        -3.10374379e-01,  1.65231586e-01, -8.03297535e-02,\n",
      "        -9.66497362e-01, -9.45455134e-01,  3.61446172e-01,\n",
      "         9.01376247e-01, -7.26962388e-02,  9.97742116e-01,\n",
      "         3.72888893e-02,  9.35991526e-01,  2.53167838e-01,\n",
      "        -2.01844558e-01,  2.95320340e-02, -2.31625080e-01,\n",
      "         3.46315920e-01, -1.07626274e-01, -2.65651494e-01,\n",
      "         1.08740889e-01,  1.29849359e-01,  2.11338159e-02,\n",
      "        -9.62828621e-02, -7.63576105e-02, -6.51485920e-02,\n",
      "        -8.92766714e-01, -2.34647542e-01,  9.11759079e-01,\n",
      "         7.04297423e-02, -2.14292213e-01,  3.81972164e-01,\n",
      "         3.58917639e-02, -1.69710100e-01,  7.06543088e-01,\n",
      "         2.40448475e-01,  1.50139540e-01, -1.94774792e-02,\n",
      "         2.13693291e-01, -1.79770380e-01,  3.51117164e-01,\n",
      "        -6.02602184e-01,  4.16827589e-01,  1.80900738e-01,\n",
      "        -3.24963368e-02, -3.01374346e-01, -9.71034765e-01,\n",
      "        -1.39173687e-01,  3.51302326e-01,  9.83260274e-01,\n",
      "         5.27020276e-01,  4.88114990e-02,  1.39899729e-02,\n",
      "        -6.79638758e-02,  2.97174543e-01, -9.41362798e-01,\n",
      "         9.72189367e-01, -2.47734115e-02,  1.52244255e-01,\n",
      "        -1.82412446e-01,  5.55832945e-02, -7.73061872e-01,\n",
      "        -9.90003943e-02,  4.70583737e-01, -1.70223966e-01,\n",
      "        -7.78029978e-01,  5.28336316e-02, -3.76786709e-01,\n",
      "        -4.12957743e-02, -4.96121407e-01,  1.41710773e-01,\n",
      "        -1.18026443e-01, -1.89949363e-01,  5.03835939e-02,\n",
      "         9.06232595e-01,  7.88281739e-01,  5.22883534e-01,\n",
      "        -3.52741092e-01,  2.85634518e-01, -8.14936697e-01,\n",
      "        -1.96219742e-01, -9.29755196e-02,  5.93107380e-02,\n",
      "         3.19020823e-02,  9.88599360e-01, -3.94522220e-01,\n",
      "         1.18666612e-01, -8.69766533e-01, -9.77891505e-01,\n",
      "        -1.48587763e-01, -7.70644069e-01, -4.06153686e-03,\n",
      "        -4.11515236e-01,  3.25780094e-01,  1.87774345e-01,\n",
      "        -2.45010450e-01,  2.66675293e-01, -7.93286622e-01,\n",
      "        -4.81326818e-01,  9.32454392e-02, -1.70099109e-01,\n",
      "         2.70425946e-01, -3.58799845e-02,  7.79727519e-01,\n",
      "         4.66964006e-01, -3.46360415e-01,  5.52382469e-02,\n",
      "         9.03124809e-01, -2.41151154e-01, -6.41996622e-01,\n",
      "         4.14414465e-01, -9.77970883e-02,  6.29828155e-01,\n",
      "        -4.17870015e-01,  9.40687239e-01,  4.92848068e-01,\n",
      "         3.60580891e-01, -8.79013240e-01, -2.67259419e-01,\n",
      "        -5.46790719e-01,  9.40106460e-04, -1.05019500e-02,\n",
      "        -4.68368322e-01,  3.11159611e-01,  3.69991124e-01,\n",
      "         1.33061334e-01,  6.40917838e-01, -3.56295168e-01,\n",
      "         8.85493219e-01, -8.90358686e-01, -9.38650966e-01,\n",
      "        -8.12151134e-01,  2.73617297e-01, -9.85664308e-01,\n",
      "         4.03624952e-01,  2.12231189e-01, -1.43158674e-01,\n",
      "        -2.45530605e-01, -2.11438656e-01, -9.47283924e-01,\n",
      "         5.08063555e-01, -9.66214910e-02,  8.55710506e-01,\n",
      "        -1.01330094e-01, -6.77683473e-01, -2.84999341e-01,\n",
      "        -8.99047077e-01, -3.35770369e-01,  8.91548693e-02,\n",
      "         3.26002181e-01, -2.64674276e-01, -9.20324624e-01,\n",
      "         3.46285731e-01,  3.34304363e-01,  2.13971496e-01,\n",
      "         3.06300726e-02,  9.38777924e-01,  9.99857366e-01,\n",
      "         9.63852406e-01,  8.31590056e-01,  6.22498751e-01,\n",
      "        -9.80552793e-01, -7.36233950e-01,  9.99857843e-01,\n",
      "        -7.83952177e-01, -9.99975026e-01, -8.78004789e-01,\n",
      "        -5.08926690e-01,  2.33992040e-02, -9.99999285e-01,\n",
      "        -6.19379617e-02,  1.95627898e-01, -9.05515790e-01,\n",
      "        -1.40078351e-01,  9.52640176e-01,  7.98368394e-01,\n",
      "        -9.99997795e-01,  7.63426244e-01,  8.36699188e-01,\n",
      "        -4.58586961e-01,  5.44102609e-01, -2.40733996e-01,\n",
      "         9.60846782e-01,  1.91643611e-01,  3.21353227e-01,\n",
      "        -1.30639253e-02,  2.45335430e-01, -5.30009449e-01,\n",
      "        -5.95380366e-01,  3.74639571e-01, -2.11892784e-01,\n",
      "         8.80242884e-01,  1.96474474e-02, -3.83485794e-01,\n",
      "        -8.47786725e-01,  1.46758696e-02, -2.83754468e-02,\n",
      "        -4.43132669e-01, -9.49663281e-01, -6.57037273e-02,\n",
      "        -7.23282248e-02,  6.59669816e-01, -1.15041077e-01,\n",
      "         2.18762875e-01, -5.52535772e-01,  9.22183245e-02,\n",
      "        -5.05828142e-01, -5.28255180e-02,  5.14254034e-01,\n",
      "        -8.95327866e-01, -1.27439305e-01,  9.78451744e-02,\n",
      "        -6.01447165e-01, -3.16512808e-02, -9.51862037e-01,\n",
      "         9.46854830e-01, -2.23413557e-01,  1.83898568e-01,\n",
      "         9.99998510e-01,  1.17554158e-01, -7.03900337e-01,\n",
      "         3.25021118e-01, -1.08981347e-02, -1.83083817e-01,\n",
      "         9.99990404e-01,  5.83762586e-01, -9.73874807e-01,\n",
      "        -3.37829202e-01,  2.96399057e-01, -2.70019531e-01,\n",
      "        -2.22427189e-01,  9.97110307e-01,  1.44224400e-02,\n",
      "         7.82692507e-02,  3.86596829e-01,  9.77872670e-01,\n",
      "        -9.85011458e-01,  8.74589264e-01, -7.22760797e-01,\n",
      "        -9.52489793e-01,  9.45666909e-01,  9.10047948e-01,\n",
      "        -5.07217109e-01, -4.90259469e-01, -1.25168666e-01,\n",
      "        -3.90758552e-02,  8.81278366e-02, -8.24815094e-01,\n",
      "         3.83008987e-01,  1.80445731e-01,  5.47962971e-02,\n",
      "         8.00407052e-01, -3.35014015e-01, -3.91151369e-01,\n",
      "         1.42331764e-01, -9.01402235e-02,  3.45850855e-01,\n",
      "         4.40439403e-01,  3.10445040e-01, -1.32800356e-01,\n",
      "        -1.36142239e-01, -3.03026289e-01, -4.87943202e-01,\n",
      "        -9.49501097e-01,  1.08873159e-01,  9.99995053e-01,\n",
      "         6.07523769e-02,  8.33735168e-02, -3.12988483e-03,\n",
      "         8.55778009e-02, -3.12877476e-01,  2.62827784e-01,\n",
      "         2.68697739e-01, -1.42669812e-01, -7.39998579e-01,\n",
      "         2.28563353e-01, -7.94415355e-01, -9.88122761e-01,\n",
      "         4.35919791e-01,  7.72292987e-02, -3.80843990e-02,\n",
      "         9.94903743e-01,  3.26154798e-01,  6.79892600e-02,\n",
      "         8.28870609e-02,  4.73904461e-01, -2.18549326e-01,\n",
      "         3.92778188e-01,  3.76641750e-02,  9.64395583e-01,\n",
      "        -1.83744371e-01,  3.92593056e-01,  4.33191597e-01,\n",
      "        -1.86176106e-01, -2.15837792e-01, -4.96101618e-01,\n",
      "        -9.70247462e-02, -8.80058289e-01,  2.49950930e-01,\n",
      "        -9.39398587e-01,  9.38270688e-01,  3.20008695e-01,\n",
      "         1.19189411e-01,  7.39586279e-02,  3.12717296e-02,\n",
      "         9.99997675e-01, -7.56313682e-01,  3.53956580e-01,\n",
      "         5.32895863e-01,  3.20356011e-01, -9.75384176e-01,\n",
      "        -4.74823594e-01, -2.33224481e-01,  3.53767127e-02,\n",
      "        -4.60592024e-02, -1.28627792e-01,  8.37977231e-02,\n",
      "        -9.51387525e-01,  3.46614793e-02,  4.52134619e-03,\n",
      "        -8.82963777e-01, -9.82999086e-01,  1.64676204e-01,\n",
      "         3.35954696e-01, -1.02174096e-01, -7.02746391e-01,\n",
      "        -4.33073729e-01, -5.41688442e-01,  1.88834697e-01,\n",
      "        -5.57967499e-02, -9.21622097e-01,  4.47904170e-01,\n",
      "        -3.52558605e-02,  2.11310938e-01, -4.62671556e-02,\n",
      "         4.16884631e-01,  1.93113402e-01,  8.26429009e-01,\n",
      "         3.18977498e-02,  1.80363823e-02,  2.25022733e-02,\n",
      "        -5.62614322e-01,  5.26899874e-01, -4.15228337e-01,\n",
      "        -2.03345940e-01,  5.09746931e-03,  9.99998629e-01,\n",
      "        -1.37685984e-01,  4.00896490e-01,  4.85805333e-01,\n",
      "         3.05470645e-01,  1.01610877e-01,  1.13717012e-01,\n",
      "         5.46874821e-01,  1.72822729e-01, -1.16108328e-01,\n",
      "         1.16916142e-01,  3.37057352e-01, -9.49953198e-02,\n",
      "         3.31252456e-01, -1.15997322e-01,  5.56628332e-02,\n",
      "         6.90172911e-01,  5.27747571e-01, -7.82482922e-02,\n",
      "         7.78744891e-02, -2.55699456e-01,  9.54414070e-01,\n",
      "         4.47251573e-02,  7.50621855e-02, -1.65209800e-01,\n",
      "         9.85724628e-02, -1.26728043e-01,  4.23959494e-01,\n",
      "         9.99990523e-01,  1.40115052e-01, -6.51178956e-02,\n",
      "        -9.86826360e-01, -3.46593320e-01, -6.95488453e-01,\n",
      "         9.99683917e-01,  7.86927998e-01, -6.25598907e-01,\n",
      "         4.05614287e-01,  5.13975263e-01, -7.19262380e-03,\n",
      "         3.74686062e-01, -4.99201976e-02, -1.83791026e-01,\n",
      "         1.06987104e-01,  6.42712638e-02,  9.43630993e-01,\n",
      "        -4.59820598e-01, -9.66839254e-01, -4.87142354e-01,\n",
      "         1.62325636e-01, -9.29821074e-01,  9.89759743e-01,\n",
      "        -2.82408953e-01, -3.95258665e-02, -2.89686918e-01,\n",
      "         2.21783102e-01, -7.33220220e-01, -1.97521061e-01,\n",
      "        -9.73850369e-01,  1.46248385e-01,  1.73838865e-02,\n",
      "         9.44592059e-01,  8.00701603e-02, -4.10258204e-01,\n",
      "        -7.23629057e-01,  6.54932484e-02,  2.95309305e-01,\n",
      "        -2.04021752e-01, -9.44529116e-01,  9.48673487e-01,\n",
      "        -9.62239981e-01,  4.19867665e-01,  9.99919891e-01,\n",
      "         2.01821148e-01, -5.97189903e-01,  6.70615733e-02,\n",
      "        -1.35596097e-01,  1.11395642e-01, -7.10702986e-02,\n",
      "         3.38431478e-01, -9.19282615e-01, -1.17846891e-01,\n",
      "         7.19030667e-03,  9.38126221e-02,  1.27179757e-01,\n",
      "        -4.21753705e-01,  6.23830795e-01, -3.09482310e-02,\n",
      "        -3.95731449e-01, -4.99114484e-01,  1.97134823e-01,\n",
      "         1.95740834e-01,  5.27740836e-01, -6.49985895e-02,\n",
      "         3.82176936e-02, -1.37642741e-01,  1.31136507e-01,\n",
      "        -8.28963161e-01, -6.28011748e-02, -1.30774915e-01,\n",
      "        -9.97451305e-01,  3.81894052e-01, -9.99998450e-01,\n",
      "        -4.95283380e-02, -3.30112696e-01, -9.70479473e-03,\n",
      "         7.40315199e-01,  4.55875427e-01, -4.30386774e-02,\n",
      "        -5.94850242e-01,  3.51389535e-02,  8.42901289e-01,\n",
      "         7.00243652e-01,  4.95038042e-03,  1.52212813e-01,\n",
      "        -4.81815964e-01,  3.49119455e-02,  6.86808303e-02,\n",
      "         5.97965755e-02,  9.41473320e-02,  5.75319648e-01,\n",
      "         3.50631811e-02,  9.99999046e-01, -4.47858684e-03,\n",
      "        -3.47573787e-01, -7.93086946e-01,  5.72407916e-02,\n",
      "        -4.82413359e-02,  9.99906838e-01, -3.69631678e-01,\n",
      "        -9.27294374e-01,  2.26102918e-01, -3.26021463e-01,\n",
      "        -6.59482658e-01,  2.35061139e-01, -6.60267547e-02,\n",
      "        -6.28751338e-01, -4.71235991e-01,  8.31054449e-01,\n",
      "         4.34616566e-01, -5.22375107e-01,  2.18105525e-01,\n",
      "        -1.11757666e-01, -2.70267636e-01, -6.85015768e-02,\n",
      "         5.05021736e-02,  9.83191788e-01,  3.38879317e-01,\n",
      "         5.64422607e-01,  1.05172783e-01,  6.14417307e-02,\n",
      "         9.36657727e-01,  7.39880279e-02, -2.45281905e-01,\n",
      "        -8.52072760e-02,  9.99975979e-01,  1.42104194e-01,\n",
      "        -8.24880481e-01,  2.24047348e-01, -9.20979798e-01,\n",
      "        -1.02346607e-01, -8.41053486e-01,  2.11401865e-01,\n",
      "        -3.41071896e-02,  8.09416354e-01,  4.98409104e-03,\n",
      "         8.96236181e-01,  6.71864450e-02, -1.71371132e-01,\n",
      "        -2.75605083e-01,  2.63850510e-01,  1.90726429e-01,\n",
      "        -8.63067567e-01, -9.82381701e-01, -9.80348468e-01,\n",
      "         2.23697066e-01, -3.51537049e-01,  1.91805959e-01,\n",
      "         8.95032883e-02, -9.81391296e-02,  8.35931599e-02,\n",
      "         3.03730071e-01, -9.99984026e-01,  9.09437537e-01,\n",
      "         2.90070385e-01,  4.45846677e-01,  9.46312606e-01,\n",
      "         4.12595779e-01,  1.96212009e-01,  2.46926501e-01,\n",
      "        -9.75619197e-01, -7.69568086e-01, -1.79963440e-01,\n",
      "        -5.86007796e-02,  4.29491013e-01,  3.33413988e-01,\n",
      "         8.05474937e-01,  2.53064066e-01, -4.07364249e-01,\n",
      "        -3.45859230e-02,  4.09996063e-01, -8.38742256e-01,\n",
      "        -9.90924358e-01,  3.09373707e-01,  3.39173406e-01,\n",
      "        -6.26788318e-01,  9.45652604e-01, -5.96134305e-01,\n",
      "        -1.94380910e-03,  3.79710317e-01, -2.22495332e-01,\n",
      "         5.21581590e-01,  5.93243837e-01, -1.83568466e-02,\n",
      "        -6.80001546e-03,  2.15540916e-01,  8.24840784e-01,\n",
      "         8.00677121e-01,  9.77952719e-01, -1.08679995e-01,\n",
      "         4.39631343e-01,  2.23877743e-01,  2.70780206e-01,\n",
      "         8.50645363e-01, -9.25673962e-01,  4.36277874e-03,\n",
      "        -3.20626758e-02, -1.95653781e-01,  1.11692861e-01,\n",
      "        -9.47105512e-02, -7.26445377e-01,  6.39858663e-01,\n",
      "        -1.79553822e-01,  4.29391474e-01, -2.07871795e-01,\n",
      "         2.22941205e-01, -2.38571554e-01,  6.71948418e-02,\n",
      "        -5.17724454e-01, -3.63893718e-01,  5.31695247e-01,\n",
      "         5.34842946e-02,  8.53086352e-01,  6.46111727e-01,\n",
      "         1.23411110e-02, -2.47556314e-01,  1.47184785e-02,\n",
      "        -5.32933548e-02, -9.25663412e-01,  5.07711172e-01,\n",
      "         1.24919735e-01,  2.14575514e-01, -6.79598674e-02,\n",
      "        -2.71126032e-01,  9.09462333e-01, -1.90319687e-01,\n",
      "        -2.12741703e-01, -6.48466423e-02, -4.38712299e-01,\n",
      "         6.37515426e-01, -2.10172221e-01, -2.92911917e-01,\n",
      "        -3.16155642e-01,  5.41166723e-01,  1.67678788e-01,\n",
      "         9.94241357e-01, -9.45078060e-02, -2.90221065e-01,\n",
      "        -2.18768674e-03, -1.57195956e-01,  2.83170491e-01,\n",
      "        -2.93637425e-01, -9.99982595e-01,  1.40657321e-01,\n",
      "         9.16058868e-02,  1.14574589e-01, -2.19647735e-01,\n",
      "         3.07460904e-01, -5.77185526e-02, -8.76921654e-01,\n",
      "        -9.38913375e-02,  2.28086233e-01,  3.87664251e-02,\n",
      "        -3.28281403e-01, -3.11384052e-01,  4.11171436e-01,\n",
      "         4.60044205e-01,  5.52659094e-01,  7.25346386e-01,\n",
      "         2.56349742e-01,  5.29581130e-01,  4.79643226e-01,\n",
      "        -1.04019158e-01, -5.42038620e-01,  8.49344909e-01]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(input_ids)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea4f7764-bf8d-4cd9-ab09-79be04c67d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fba53775-9828-4d91-8225-9e3b6fa7595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.11437128  0.19371368  0.1249586  ... -0.38269076  0.21065912\n",
      "    0.54070807]\n",
      "  [ 0.53082436  0.32074896  0.3664591  ... -0.00360684  0.7578602\n",
      "    0.0388434 ]\n",
      "  [-0.48765117  0.88492435  0.42556354 ... -0.697621    0.44583344\n",
      "    0.12309441]\n",
      "  ...\n",
      "  [-0.70027906 -0.18150637  0.32969648 ... -0.48379344  0.06802348\n",
      "    0.890084  ]\n",
      "  [-1.0354631  -0.2566779  -0.03165286 ...  0.31974417  0.39990202\n",
      "    0.17954747]\n",
      "  [ 0.6079918   0.2609707  -0.31307253 ...  0.03109779 -0.6282723\n",
      "   -0.19942449]]], shape=(1, 8, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0b8455d-8c98-4441-a31d-96a2c7a08943",
   "metadata": {},
   "outputs": [],
   "source": [
    "clse = last_hidden_states[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "819f5685-8d5e-4c9e-ae17-bf91f1a39699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clse.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f564402-b935-4b06-ab3d-256875d25744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(768,), dtype=float32, numpy=\n",
       "array([-1.14371277e-01,  1.93713680e-01,  1.24958605e-01, -2.03833938e-01,\n",
       "       -1.43783808e-01, -4.01328981e-01,  3.77295054e-02,  2.35692024e-01,\n",
       "       -6.07896745e-02, -1.99522629e-01,  5.90228811e-02,  3.77066880e-02,\n",
       "        3.21126491e-01,  4.20141160e-01, -2.22825021e-01, -5.37456945e-04,\n",
       "       -2.28127822e-01,  5.35356939e-01, -8.61098990e-03,  1.03082299e-01,\n",
       "        3.26674841e-02, -1.71210825e-01, -1.52831823e-01, -2.00476810e-01,\n",
       "        8.08305815e-02,  6.31937832e-02,  1.06357351e-01,  2.71277726e-01,\n",
       "       -5.52153178e-02,  1.35896146e-01, -1.47843897e-01,  8.43284428e-02,\n",
       "        4.82296288e-01,  5.44626676e-02,  2.98799843e-01, -1.00577407e-01,\n",
       "        1.90387905e-01,  1.64552256e-02, -4.87703085e-01,  5.64810783e-02,\n",
       "       -7.77095705e-02,  5.17870579e-03,  1.72790978e-02, -1.61808014e-01,\n",
       "       -2.03584030e-01, -2.50444412e-01, -3.30151510e+00, -5.04574627e-02,\n",
       "       -2.23637670e-02, -5.34201443e-01,  3.79213661e-01, -3.47824067e-01,\n",
       "       -2.68439710e-01,  1.25216275e-01,  2.27945656e-01,  2.17541426e-01,\n",
       "       -3.70355904e-01,  7.21950382e-02,  1.62042767e-01,  4.02856395e-02,\n",
       "        2.25337952e-01,  1.21297747e-01, -1.22957118e-02,  9.40062255e-02,\n",
       "       -1.49411976e-01,  1.06106892e-01,  3.14753383e-01,  2.97312081e-01,\n",
       "       -2.11096078e-01,  8.70639563e-01, -3.31735373e-01, -8.62169787e-02,\n",
       "        7.81807005e-02,  1.33405685e-01,  6.27735481e-02, -2.78036654e-01,\n",
       "        2.10205168e-01,  1.45136356e-01, -3.10982883e-01,  2.17591763e-01,\n",
       "       -6.20089471e-02,  6.62921667e-02,  4.69646752e-01,  7.81403482e-03,\n",
       "        9.94873494e-02,  3.50650787e-01, -1.35266125e-01, -4.03216273e-01,\n",
       "        7.75546655e-02,  3.42415988e-01,  6.59511536e-02, -1.58423230e-01,\n",
       "       -1.38132989e-01,  2.42140025e-01,  3.36165667e-01,  1.44351006e-01,\n",
       "       -3.45179766e-01,  3.80781502e-01, -2.06181332e-02, -6.34231195e-02,\n",
       "       -2.75107205e-01, -1.29768640e-01,  8.39857310e-02, -2.01410875e-01,\n",
       "        1.16387933e-01, -2.89555728e-01,  3.59759659e-01, -4.11922276e-01,\n",
       "       -6.82668984e-02, -2.38796329e+00,  2.33984947e-01, -1.24700367e-01,\n",
       "       -9.81374979e-02, -3.10224712e-01, -1.03783861e-01,  1.05873609e+00,\n",
       "        4.73727882e-01, -1.24002725e-01, -3.54453921e-04,  4.49954033e-01,\n",
       "       -7.27546662e-02,  1.04079366e-01,  1.27213717e-01, -2.73861766e-01,\n",
       "        4.05893952e-01,  2.97598451e-01,  2.16814298e-02, -1.16508678e-01,\n",
       "        3.60765815e-01, -1.73102960e-01,  1.24291316e-01,  4.38904226e-01,\n",
       "        2.39069164e-01, -1.26906246e-01, -2.45988816e-01,  2.48974293e-01,\n",
       "        2.31085807e-01, -3.05934787e-01,  3.13392505e-02, -4.10178185e-01,\n",
       "       -4.43892181e-01,  3.38171907e-02, -3.07978654e+00,  4.91743207e-01,\n",
       "        4.26735461e-01,  2.79605538e-01, -3.37188125e-01,  3.94847482e-01,\n",
       "       -3.30155700e-01,  2.66548902e-01,  6.93234652e-02,  4.11582977e-01,\n",
       "       -2.56726563e-01,  1.37545034e-01, -1.68139800e-01, -2.25150928e-01,\n",
       "        1.66584942e-02, -2.61313736e-01,  1.79024428e-01,  4.87856030e-01,\n",
       "       -4.95728105e-02, -1.99652791e-01, -3.09901863e-01, -9.29281488e-02,\n",
       "       -2.39520371e-01,  4.88426089e-02,  6.18668675e-01,  3.76569927e-01,\n",
       "        7.77586326e-02, -1.69910103e-01,  1.29994825e-01,  1.43499449e-01,\n",
       "        5.24190843e-01,  2.63152242e-01,  2.18199447e-01, -6.49718940e-03,\n",
       "        5.26472270e-01, -1.70928463e-01, -1.25690833e-01,  1.42196849e-01,\n",
       "       -3.13871264e-01,  1.34875998e-03, -1.12132333e-01,  1.83479935e-01,\n",
       "        3.76998097e-01, -1.13688260e-01,  3.12558979e-01, -1.04696602e-01,\n",
       "       -1.11319259e-01, -9.63227302e-02, -1.35116443e-01, -2.68802822e-01,\n",
       "        1.22763164e-01,  1.04740664e-01,  4.40862358e-01,  9.25835073e-02,\n",
       "        4.11664397e-01, -4.33690548e-01,  2.44773939e-01,  1.92500323e-01,\n",
       "        1.11155957e-01, -3.03228199e-01,  2.14474857e-01,  3.53967309e-01,\n",
       "       -1.40419185e-01,  3.72475910e+00, -2.86697865e-01,  2.24192962e-02,\n",
       "        1.01087242e-01,  4.10891443e-01, -2.43915409e-01,  2.67234743e-01,\n",
       "       -2.69001797e-02,  1.57824501e-01, -5.94353005e-02,  6.96968287e-04,\n",
       "        6.13505721e-01, -3.91125642e-02, -6.52540267e-01,  4.94817853e-01,\n",
       "        1.51817977e-01,  7.85957351e-02, -3.86154383e-01,  5.17463028e-01,\n",
       "       -1.94975689e-01, -4.41023149e-02, -4.70119566e-02, -2.00869337e-01,\n",
       "       -2.77588610e-04, -1.27681124e+00, -1.60392582e-01, -4.27541345e-01,\n",
       "       -1.89710334e-01,  2.83604294e-01, -3.38293016e-01,  3.04589897e-01,\n",
       "       -8.83985236e-02, -5.54572105e-01,  2.36755073e-01,  3.32442462e-01,\n",
       "        1.74499661e-01,  5.34043014e-01,  5.10564387e-01,  2.32256219e-01,\n",
       "       -2.09300503e-01,  2.74440140e-01,  4.18261230e-01, -4.86330166e-02,\n",
       "        3.52075279e-01,  3.70653905e-03,  2.56903350e-01,  1.54158883e-02,\n",
       "        3.65881711e-01, -3.09597909e-01,  2.25551695e-01, -4.90752868e-02,\n",
       "       -1.84153356e-02,  8.74781609e-03, -2.26081520e-01,  1.42892689e-01,\n",
       "       -1.34706303e-01, -6.86814636e-02,  4.96731311e-01, -1.47333622e-01,\n",
       "       -4.57605720e-01,  3.76203418e-01,  1.95225209e-01, -5.54223835e-01,\n",
       "       -1.83718726e-02,  9.04331878e-02,  2.02437311e-01, -7.88993090e-02,\n",
       "       -4.28199977e-01, -3.44764686e+00, -9.77542400e-02,  1.25616342e-01,\n",
       "        3.84832799e-01,  3.47315013e-01,  1.08849332e-01, -3.84740651e-01,\n",
       "        4.61166382e-01,  9.23877582e-02, -7.47164488e-01,  1.72931910e-01,\n",
       "        5.62634841e-02, -3.90683442e-01,  8.93222690e-02, -1.91000149e-01,\n",
       "        2.02127725e-01, -1.85994692e-02, -1.56122237e-01, -4.73251045e-02,\n",
       "        4.17481661e-02,  4.63244766e-01,  1.62965477e-01, -1.76848453e-02,\n",
       "        5.57199299e-01,  1.13765240e-01, -1.65909663e-01, -1.21789813e-01,\n",
       "       -6.04859442e-02,  3.40022445e-01,  1.22855395e-01, -2.83477642e-02,\n",
       "       -1.71330899e-01,  1.92108378e-01, -3.06766778e-02, -6.08225822e-01,\n",
       "       -3.48375726e+00, -1.83671728e-01, -8.98706168e-02, -3.94816071e-01,\n",
       "       -2.16175802e-03,  1.29696816e-01,  4.14805591e-01, -1.51231468e-01,\n",
       "       -1.61229342e-01,  3.98707032e-01,  2.84269422e-01,  2.02595681e-01,\n",
       "        3.83540630e-01, -1.78774923e-01,  2.67629325e-01, -3.60594511e-01,\n",
       "        2.10037380e-01,  3.87914851e-02,  1.79476231e-01, -1.20252505e-01,\n",
       "       -2.38647819e-01,  4.78357524e-01,  7.84547403e-02, -1.09790333e-01,\n",
       "        2.69162506e-01,  5.47328889e-01, -1.10679768e-01, -2.37979397e-01,\n",
       "       -1.94866210e-01, -1.42953336e-01,  3.12355667e-01, -2.44875029e-02,\n",
       "        2.33470738e-01,  9.18026939e-02, -1.71839327e-01, -6.44717515e-02,\n",
       "        1.76248491e-01,  6.88652471e-02,  7.97281384e-01,  2.17280924e-01,\n",
       "        5.16359687e-01,  2.28234917e-01,  1.41569540e-01,  2.48581886e-01,\n",
       "        1.27968848e-01, -1.50311310e-02,  2.45922282e-02, -1.92113787e-01,\n",
       "        1.23445302e-01, -7.40204751e-03, -1.06600411e-01, -1.00663565e-02,\n",
       "        9.67502892e-01, -2.78390169e-01,  3.42207283e-01, -5.94065413e-02,\n",
       "        2.36202367e-02,  2.63756216e-01, -1.76104158e-01,  2.68872917e-01,\n",
       "        7.93274999e-01, -5.16621590e-01,  4.73523796e-01, -4.66091111e-02,\n",
       "       -1.24618478e-01, -5.55224158e-02,  1.20193228e-01, -5.46921313e-01,\n",
       "       -4.69542034e-02,  2.41883010e-01,  2.75445670e-01,  9.21337605e-02,\n",
       "       -9.26240906e-02, -1.21594596e+00, -1.88151851e-01,  3.29243213e-01,\n",
       "       -3.89059871e-01,  1.54246509e-01,  7.80800432e-02, -2.19046488e-01,\n",
       "       -1.81873649e-01, -2.00003058e-01,  2.71982551e-02,  8.61362815e-01,\n",
       "       -7.98924148e-01, -1.32363290e-01, -2.67040938e-01, -5.61923444e-01,\n",
       "       -2.85937905e-01,  6.49514794e-03, -3.80623996e-01, -1.50865316e-03,\n",
       "       -2.16671437e-01,  3.06265861e-01,  4.17701304e-02,  7.85467699e-02,\n",
       "       -5.94177395e-02, -8.90979767e-01, -4.07728925e-02, -1.68885544e-01,\n",
       "       -5.03404438e-03,  1.20531589e-01,  6.02108240e-02,  2.30681330e-01,\n",
       "        2.68826097e-01, -1.65838674e-02, -4.62768406e-01,  3.37206274e-01,\n",
       "        2.58559912e-01,  2.10895017e-01,  7.45987967e-02, -2.80534029e-01,\n",
       "       -8.33521411e-02,  1.19366869e-01,  7.28080153e-01, -2.04147249e-01,\n",
       "        1.50972992e-01,  6.68543994e-01, -2.99295969e-02,  3.27741861e-01,\n",
       "        5.29381298e-02,  2.52452996e-02,  1.52669214e-02, -7.26707280e-03,\n",
       "       -3.58757496e-01, -7.61621073e-02, -7.66982809e-02, -3.03699166e-01,\n",
       "       -5.85282147e-01, -1.55915976e-01,  4.36587453e-01,  6.74905479e-02,\n",
       "       -4.50279176e-01, -5.88328362e-01,  1.93931818e-01, -1.69998214e-01,\n",
       "       -3.58650774e-01, -6.50358051e-02,  2.45002866e-01,  1.46910995e-01,\n",
       "       -9.16084722e-02, -2.50604808e-01, -3.18327755e-01,  2.89781898e-01,\n",
       "       -4.93562520e-02,  4.25511181e-01, -4.02009189e-02,  1.30247280e-01,\n",
       "        6.58571720e-02,  4.65394616e-01,  1.75474182e-01, -5.26447058e-01,\n",
       "        9.07966867e-02, -3.16086441e-01,  3.55167240e-01,  2.44577348e-01,\n",
       "        1.98467344e-01, -1.57758206e-01, -1.75969839e-01,  1.18646197e-01,\n",
       "        1.54053032e-01, -1.93900079e-01, -1.17571855e+00,  5.34699500e-01,\n",
       "        7.67968744e-02,  1.40045881e-01,  2.06999958e-01,  3.47215608e-02,\n",
       "       -1.78745359e-01,  2.38687247e-01,  2.79142171e-01,  7.15155154e-02,\n",
       "       -1.58520371e-01,  3.45416307e-01,  1.72535628e-02,  2.70470619e-01,\n",
       "        1.09173119e-01,  9.70606506e-02,  2.34140381e-01, -4.85555112e-01,\n",
       "        1.41430825e-01, -1.30592227e-01,  5.72245046e-02,  3.89183164e-01,\n",
       "        2.76450843e-01,  1.13098927e-01,  1.78948224e-01, -2.56002754e-01,\n",
       "        1.59716666e-01,  5.69181561e-01,  7.32742921e-02,  3.80643040e-01,\n",
       "        3.33144367e-01, -1.59509525e-01, -4.57814932e-01, -6.49477094e-02,\n",
       "        1.13656998e-01, -1.93681613e-01,  3.89188081e-02, -8.52661580e-02,\n",
       "        1.46926150e-01,  6.56015202e-02, -2.30460912e-01,  2.52201885e-01,\n",
       "        1.31405562e-01, -2.98254400e-01,  2.21188009e-01,  7.05374107e-02,\n",
       "       -2.35589504e-01,  3.09383810e-01,  4.46048141e-01, -5.59181273e-01,\n",
       "       -1.91161603e-01, -5.53311765e-01, -1.59516096e-01,  3.77803072e-02,\n",
       "       -8.16811845e-02, -2.08484605e-02,  3.75357196e-02, -2.68133610e-01,\n",
       "       -4.43253577e-01, -2.20223978e-01,  1.91759378e-01, -1.65051952e-01,\n",
       "       -3.77170444e-01, -2.37811148e-01, -5.43124080e-01, -5.63669264e-01,\n",
       "       -9.87791568e-02, -4.37588722e-01,  8.70421827e-02,  4.12921347e-02,\n",
       "        3.76050711e-01,  1.72613487e-02, -3.06520551e-01,  2.39215851e-01,\n",
       "       -4.92728859e-01,  1.24936245e-01,  5.40860891e-02,  4.85932708e-01,\n",
       "        3.76568735e-01, -2.64019132e-01, -8.14281255e-02, -7.04514146e-01,\n",
       "       -3.10024381e-01,  4.42059845e-01, -2.86724687e-01,  2.93535560e-01,\n",
       "        1.00557320e-01,  2.15251803e-01,  6.26920089e-02, -8.50004852e-02,\n",
       "       -3.05026203e-01, -4.11707908e-01,  2.27547273e-01,  1.83378160e-02,\n",
       "       -3.07072371e-01,  1.56585649e-01, -1.41055420e-01,  1.26537979e-01,\n",
       "       -5.32947108e-02,  1.45712525e-01, -9.53878760e-02,  3.34485650e-01,\n",
       "        2.50336170e-01,  2.35457495e-02,  5.21780131e-03,  2.57027626e-01,\n",
       "        4.17386740e-01,  2.28991181e-01, -5.99301040e-01, -4.56772208e-01,\n",
       "        4.61847216e-01, -5.47782332e-02, -1.69448331e-01, -1.37072839e-02,\n",
       "        2.53295124e-01, -3.42937648e-01,  5.15610218e-01, -2.52906561e-01,\n",
       "        2.15570593e+00,  4.13514465e-01, -4.92699668e-02, -1.57281205e-01,\n",
       "        4.56980050e-01, -1.11501634e-01, -5.09614468e-01, -6.65597916e-02,\n",
       "       -3.38330090e-01,  5.67728102e-01,  4.98997644e-02,  3.13877583e-01,\n",
       "       -1.15786098e-01, -4.14400101e-02,  5.49709976e-01,  4.75772202e-01,\n",
       "       -1.78042740e-01, -3.52186374e-02, -1.11751783e+00, -1.01380073e-01,\n",
       "       -3.58571470e-01,  1.50816604e-01,  5.02629817e-01, -1.90778911e-01,\n",
       "        4.30587716e-02,  2.53648341e-01,  2.60804534e-01,  5.43868989e-02,\n",
       "       -2.75904596e-01,  1.56366587e-01, -2.50795901e-01, -4.56169933e-01,\n",
       "        1.57252595e-01,  4.06713575e-01, -3.81789714e-01,  1.12667993e-01,\n",
       "        1.21127032e-01, -1.15805663e-01, -1.42639697e-01,  1.40574798e-01,\n",
       "        1.78347513e-01, -3.54168266e-01,  3.76958430e-01,  2.15238139e-01,\n",
       "       -1.61693767e-02,  7.50897467e-01, -3.61954808e-01,  6.07895702e-02,\n",
       "        3.34853292e-01,  3.61818850e-01, -4.84934747e-01, -4.26042974e-02,\n",
       "       -1.29772685e-02,  7.50441998e-02,  2.31583059e-01, -1.27679572e-01,\n",
       "        7.25151449e-02, -5.21206558e-01, -5.84880650e-01,  3.68727803e-01,\n",
       "       -4.42633927e-01,  3.96231681e-01,  6.40254319e-02,  1.38934836e-01,\n",
       "        1.74435545e-02, -4.87519354e-02, -2.61048436e-01, -2.02399701e-01,\n",
       "       -1.03088394e-01, -1.25981152e-01, -2.07782030e-01,  4.35879111e-01,\n",
       "       -7.31955841e-02, -9.72600430e-02,  1.42473906e-01,  2.35189185e-01,\n",
       "        2.97129393e-01, -4.37575608e-01, -2.86425412e-01, -3.03595853e+00,\n",
       "        9.83536243e-03,  1.44795002e-03, -3.10164578e-02,  9.66879278e-02,\n",
       "        1.95613429e-01,  3.56732696e-01,  4.80154864e-02,  1.72615707e-01,\n",
       "       -2.54377872e-02,  3.72419983e-01,  3.91122326e-02,  6.14068210e-01,\n",
       "        6.38323277e-03,  2.70054042e-01, -6.45362511e-02,  1.24329418e-01,\n",
       "       -3.74493003e-01, -2.11705208e-01, -1.93398818e-01, -5.47602922e-02,\n",
       "       -9.34172794e-02,  3.55079677e-03, -2.34159842e-01, -4.60886836e-01,\n",
       "        2.74017990e-01, -8.55825171e-02, -1.45527542e-01, -1.85860634e-01,\n",
       "        4.84787896e-02, -3.16465110e-01,  5.94459474e-01, -4.90613818e-01,\n",
       "        4.13391143e-02, -2.36169755e-01,  5.66837378e-03, -3.02953005e-01,\n",
       "        3.83780450e-01,  9.59081650e-02,  3.62103760e-01, -1.79154053e-01,\n",
       "        5.76752901e-01,  8.60001892e-03,  2.97174305e-01, -9.54710543e-02,\n",
       "       -2.55900234e-01,  1.16993025e-01, -6.90014474e-03,  2.02001348e-01,\n",
       "        7.38367811e-03,  2.31739059e-01,  4.21486571e-02,  4.82472003e-01,\n",
       "        3.58547986e-01,  2.16460019e-01,  2.20588103e-01,  6.75218254e-02,\n",
       "        1.15641117e-01,  8.24947208e-02, -3.59851837e-01,  8.25769231e-02,\n",
       "        3.73991132e-01, -2.40220189e-01,  1.53829917e-01,  1.04902484e-01,\n",
       "       -2.57022887e-01, -2.30713561e-01,  6.37155920e-02, -9.17307734e-02,\n",
       "       -2.70500183e-01, -7.51738250e-02, -1.23163342e-01,  3.36374253e-01,\n",
       "        5.99456951e-04,  8.96436572e-02, -7.29796439e-02,  1.69551790e-01,\n",
       "        2.49229401e-01, -1.47939682e-01,  1.67867079e-01, -3.45658511e-02,\n",
       "       -1.18971989e-01, -3.25306743e-01, -1.42158121e-01,  2.31369793e-01,\n",
       "       -7.38645363e+00,  2.64270872e-01, -7.40589350e-02, -3.03844929e-01,\n",
       "       -3.37450057e-01, -5.63442647e-01, -2.07102373e-01, -1.04138657e-01,\n",
       "        2.44592145e-01, -8.66344199e-02,  3.38529617e-01, -6.30367249e-02,\n",
       "       -4.24776912e-01, -3.82690758e-01,  2.10659117e-01,  5.40708065e-01],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b286c311-123e-4664-828d-fb7024392eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of encoded string(including special tokens such as [CLS] and [SEP]):\n",
    "MAX_SEQUENCE_LENGTH = 64 \n",
    "\n",
    "# Standard BERT model with lowercase chars only:\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased' \n",
    "\n",
    "# Batch size for fitting:\n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "# Number of epochs:\n",
    "EPOCHS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "edae9553-606b-4ae3-9a04-0071c5bd279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence, model_name, num_labels):\n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    # This is the input for the tokens themselves(words from the dataset after encoding):\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence,), dtype=tf.int32, name='input_ids')\n",
    "\n",
    "    # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.\n",
    "    # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH, \n",
    "    # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad token:\n",
    "    attention_mask = tf.keras.layers.Input((max_sequence,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # Use previous inputs as BERT inputs:\n",
    "    output = bert_model([input_ids, attention_mask])[0]\n",
    "    \n",
    "    print(output)\n",
    "\n",
    "    # We can also add dropout as regularization technique:\n",
    "    #output = tf.keras.layers.Dropout(rate=0.15)(output)\n",
    "\n",
    "    # Provide number of classes to the final layer:\n",
    "    output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)\n",
    "\n",
    "    # Final model:\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "16983aa3-d681-4675-be29-25f3731b89cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2), dtype=tf.float32, name=None), name='tf_bert_for_sequence_classification_7/classifier/BiasAdd:0', description=\"created by layer 'tf_bert_for_sequence_classification_7'\")\n"
     ]
    }
   ],
   "source": [
    "model = create_model(MAX_SEQUENCE_LENGTH, PRETRAINED_MODEL_NAME, 2)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "025cb6bf-7858-4c4d-9d82-c4893bd0a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode(X, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "    X,\n",
    "    max_length=MAX_SEQUENCE_LENGTH, # set the length of the sequences\n",
    "    add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False, # not needed for this type of ML task\n",
    "    pad_to_max_length=True, # add 0 pad tokens to the sequences less than max_length\n",
    "    return_tensors='tf',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b3ca5082-bf64-4893-9df6-19377c25608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4009d2ff-49ed-4bea-b1b4-4168f613e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034acf96-b6fb-427f-a71a-bc7364a00b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sentence, sentence]\n",
    "y_train = np.array([1, 0])\n",
    "X_val = [sentence, sentence]\n",
    "y_val = np.array([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dcbc159-ba53-4d9a-bb02-2cd2fd9b8137",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\BERT.ipynb Cell 63'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000062?line=0'>1</a>\u001b[0m X_train \u001b[39m=\u001b[39m batch_encode(X_train, tokenizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000062?line=1'>2</a>\u001b[0m X_val \u001b[39m=\u001b[39m batch_encode(X_val, tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_encode' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = batch_encode(X_train, tokenizer)\n",
    "X_val = batch_encode(X_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8b214f06-149b-405d-bc58-d87994178028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([<tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n",
       "array([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]])>, <tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "381a51ab-a719-424e-8c3c-ea2ca299b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_for_sequence_classific  TFSequenceClassifie  109483778  ['input_ids[0][0]',              \n",
      " ation_2 (TFBertForSequenceClas  rOutput(loss=None,               'attention_mask[0][0]']         \n",
      " sification)                    logits=(None, 2),                                                 \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            6           ['tf_bert_for_sequence_classifica\n",
      "                                                                 tion_2[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,784\n",
      "Trainable params: 109,483,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f1ae608-f7f5-4267-8346-a1ef27320e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 20s 20s/step - loss: 0.7146 - accuracy: 0.5000 - val_loss: 0.7016 - val_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6968 - accuracy: 0.5000 - val_loss: 0.6997 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7168 - accuracy: 0.5000 - val_loss: 0.6976 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7327 - accuracy: 0.5000 - val_loss: 0.6988 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6997 - accuracy: 0.5000 - val_loss: 0.6991 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ed1f0e2b20>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=X_train.values(),\n",
    "    y=y_train,\n",
    "    validation_data=(X_val.values(), y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "691a7ead-e84c-4c17-bafe-6d5e8a20ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.7734848], dtype=float32)>, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.00591445, -0.14878385]], dtype=float32)>, hidden_states=(<tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.4495986 ,  0.09766434, -0.20736787, ...,  0.05780864,\n",
      "          0.04061905, -0.09512451],\n",
      "        [-0.77356714,  0.49493614,  0.40814084, ...,  0.11958291,\n",
      "         -0.31714237,  0.25474828],\n",
      "        [-1.143637  ,  0.41187298, -0.28890803, ...,  0.5126003 ,\n",
      "         -1.2320071 , -0.52256787],\n",
      "        ...,\n",
      "        [-0.14134963, -1.0802779 ,  0.8814317 , ...,  0.5289587 ,\n",
      "         -0.5264092 ,  0.6608591 ],\n",
      "        [ 0.20395406, -1.2452701 ,  0.910847  , ...,  0.40185875,\n",
      "         -0.5353736 ,  0.82868004],\n",
      "        [-0.04057743, -1.1146938 ,  0.71299577, ...,  0.28208214,\n",
      "         -0.648682  ,  0.5204784 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.2717536 , -0.04192287, -0.09590326, ...,  0.04059108,\n",
      "          0.04587968, -0.06677815],\n",
      "        [-1.0057184 ,  0.8226264 ,  0.08506905, ...,  0.0218928 ,\n",
      "         -0.17773648, -0.13762966],\n",
      "        [-0.6432052 ,  0.01664471, -0.18631639, ...,  0.00367083,\n",
      "         -1.47046   , -0.48410627],\n",
      "        ...,\n",
      "        [-0.82927746,  0.28863037,  0.72361857, ..., -0.57800734,\n",
      "          0.1307468 , -0.05393491],\n",
      "        [-0.5045121 ,  0.0227115 ,  0.6665975 , ..., -0.6014166 ,\n",
      "          0.076334  ,  0.18229644],\n",
      "        [-0.78299576,  0.2026111 ,  0.5040815 , ..., -0.7440822 ,\n",
      "          0.07636045,  0.00765086]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.4113537 ,  0.03542183, -0.13542114, ...,  0.02569922,\n",
      "          0.09557049, -0.21680173],\n",
      "        [-1.5705411 ,  0.7848764 ,  0.64523697, ...,  0.05660557,\n",
      "         -0.0944061 ,  0.2905097 ],\n",
      "        [-0.32248846,  0.21803924,  0.01887472, ...,  0.06627898,\n",
      "         -1.6043357 , -0.4186162 ],\n",
      "        ...,\n",
      "        [-0.51147556,  0.32392716, -0.00294674, ..., -0.6548061 ,\n",
      "          0.37816164, -0.38762832],\n",
      "        [-0.17566144,  0.19532612,  0.06606326, ..., -0.74281734,\n",
      "          0.418076  , -0.14147389],\n",
      "        [-0.52168953,  0.14602672, -0.09764053, ..., -0.6127122 ,\n",
      "          0.41315353, -0.17275095]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.747951  ,  0.27842423, -0.02599991, ..., -0.11867957,\n",
      "          0.20770487,  0.05830685],\n",
      "        [-0.864117  ,  0.5627613 ,  0.6533158 , ..., -0.5003373 ,\n",
      "          0.23912533,  0.76711833],\n",
      "        [-0.03846994,  0.35835725,  0.26583165, ..., -0.24068788,\n",
      "         -1.1931301 , -0.07848042],\n",
      "        ...,\n",
      "        [-0.36296296,  0.3771789 ,  0.03935151, ..., -0.5116693 ,\n",
      "          0.27092746, -0.5614311 ],\n",
      "        [-0.21564241,  0.28091723,  0.09653701, ..., -0.43084592,\n",
      "          0.40868828, -0.61329615],\n",
      "        [-0.5051213 ,  0.27645755, -0.00122335, ..., -0.15878648,\n",
      "          0.54011446, -0.3486855 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 1.0046226 ,  0.13807979, -0.19961499, ..., -0.20288283,\n",
      "          0.3202954 , -0.13604575],\n",
      "        [-0.6251106 , -0.23203014,  0.61730826, ..., -0.57891756,\n",
      "          0.3137484 ,  1.0665734 ],\n",
      "        [ 0.09344125,  0.7069855 ,  0.28426725, ...,  0.3439204 ,\n",
      "         -1.5786818 ,  0.1838922 ],\n",
      "        ...,\n",
      "        [-0.42478514, -0.04417733,  0.17523752, ..., -0.57372475,\n",
      "          0.36609566, -0.20999777],\n",
      "        [-0.2135917 ,  0.3937164 ,  0.29970932, ..., -0.2148481 ,\n",
      "          0.46469384, -0.651316  ],\n",
      "        [-0.6423254 ,  0.12481614,  0.04504028, ..., -0.17138043,\n",
      "          0.96727234, -0.04640239]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.65392536,  0.07504879, -0.31819898, ...,  0.13731223,\n",
      "          0.22386964,  0.18493047],\n",
      "        [-0.5588591 , -0.3727954 ,  0.53505117, ..., -0.8424486 ,\n",
      "          0.5102325 ,  0.7580971 ],\n",
      "        [-0.44435364,  0.67128485,  0.17989384, ...,  0.35587835,\n",
      "         -1.310087  ,  0.40854365],\n",
      "        ...,\n",
      "        [-0.726029  ,  0.05631098,  0.4572392 , ..., -0.22703652,\n",
      "          0.48195451, -0.26570398],\n",
      "        [-0.7141562 ,  0.22714898,  0.12022512, ...,  0.16720137,\n",
      "          0.4961822 , -0.5657394 ],\n",
      "        [-0.9107365 ,  0.169726  ,  0.11996478, ...,  0.3050424 ,\n",
      "          0.9599275 , -0.15234566]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.6287231 ,  0.09801637, -0.24016707, ...,  0.08790691,\n",
      "          0.2347407 ,  0.09451795],\n",
      "        [-1.1530353 , -0.86748075,  0.14664774, ...,  0.10651734,\n",
      "          0.24783684,  0.57126683],\n",
      "        [-0.48547286,  0.92328507,  0.06264733, ...,  0.76924384,\n",
      "         -1.3928778 ,  0.8748554 ],\n",
      "        ...,\n",
      "        [-0.6068442 , -0.0624581 ,  0.5005005 , ...,  0.08372362,\n",
      "          0.6212526 ,  0.06608756],\n",
      "        [-0.52677745,  0.21867582,  0.22750771, ...,  0.61828697,\n",
      "          0.42393556,  0.03618112],\n",
      "        [-0.6778434 ,  0.11327402,  0.21741693, ...,  0.65787745,\n",
      "          1.3413128 ,  0.10726638]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.41759467,  0.27723294, -0.43783355, ..., -0.13128594,\n",
      "         -0.16570257, -0.03024263],\n",
      "        [-1.0239758 , -0.9478185 ,  0.04848468, ..., -0.2207675 ,\n",
      "          0.4865424 ,  0.37934306],\n",
      "        [-0.29349983,  1.181611  , -0.0791633 , ...,  0.11689858,\n",
      "         -0.9054741 ,  0.8016735 ],\n",
      "        ...,\n",
      "        [-0.6416546 ,  0.07965674,  0.3929208 , ..., -0.19310921,\n",
      "          0.7803275 ,  0.02712587],\n",
      "        [-0.13811994,  0.6276928 ,  0.05473451, ...,  0.28276286,\n",
      "          0.46254742,  0.11691382],\n",
      "        [-0.25678653,  0.36273727, -0.06770976, ...,  0.29609576,\n",
      "          1.5306716 ,  0.0533267 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.14264825,  0.47637373, -0.22101593, ..., -0.42810777,\n",
      "         -0.20844232, -0.11585789],\n",
      "        [-0.63882095, -0.70646167, -0.03192337, ..., -0.66733783,\n",
      "          0.61210364,  0.5088238 ],\n",
      "        [ 0.04127431,  1.4510192 ,  0.0680201 , ...,  0.45462894,\n",
      "         -1.2131885 ,  0.96490586],\n",
      "        ...,\n",
      "        [-0.6114702 ,  0.2410463 ,  0.24431548, ..., -0.20327447,\n",
      "          0.66148573,  0.18824238],\n",
      "        [ 0.1301331 ,  1.1221621 ,  0.12672594, ...,  0.3348631 ,\n",
      "          0.29749873, -0.19243051],\n",
      "        [-0.2000985 ,  0.5785368 , -0.3603623 , ...,  0.20285818,\n",
      "          1.238944  ,  0.33241734]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.3579655 ,  0.7665882 , -0.01365671, ..., -0.56607056,\n",
      "          0.18296602, -0.4623926 ],\n",
      "        [-0.44317377, -0.6184645 ,  0.29880103, ..., -0.3879462 ,\n",
      "          0.50778836,  0.4666363 ],\n",
      "        [ 0.2601076 ,  1.5714998 ,  0.9411452 , ...,  0.63134414,\n",
      "         -1.3295022 ,  1.0811528 ],\n",
      "        ...,\n",
      "        [-0.38738772,  0.43292272,  0.6377402 , ..., -0.2606183 ,\n",
      "          0.49682635, -0.07754751],\n",
      "        [-0.0060748 ,  1.5419927 ,  0.716313  , ...,  0.54585487,\n",
      "          0.26095492, -0.29071754],\n",
      "        [ 0.12226446,  0.75977093,  0.03050062, ...,  0.6707242 ,\n",
      "          0.98377   ,  0.14872758]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.35562766,  1.0946242 , -0.16056128, ..., -0.31538537,\n",
      "          0.6490852 , -0.43823346],\n",
      "        [ 0.36815062, -0.925473  ,  0.5047371 , ..., -0.39709556,\n",
      "          0.28011313,  0.48919666],\n",
      "        [ 1.3953726 ,  1.0439694 ,  1.061358  , ...,  0.8717776 ,\n",
      "         -1.2489814 ,  0.9872713 ],\n",
      "        ...,\n",
      "        [-0.05743501,  0.24588674,  0.7747303 , ..., -0.38251096,\n",
      "          0.46248367,  0.13517934],\n",
      "        [ 0.35850826,  1.3656436 ,  0.5444269 , ...,  0.5006408 ,\n",
      "          0.22670099, -0.19239989],\n",
      "        [ 0.4438493 ,  0.46479774,  0.15534802, ...,  0.8052641 ,\n",
      "          0.8811157 ,  0.31277686]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.4629375 ,  1.1814607 ,  0.27575207, ..., -0.2780588 ,\n",
      "          0.73968107, -0.29886192],\n",
      "        [ 0.3228344 , -0.65143245,  1.0510073 , ..., -0.42321533,\n",
      "          0.24877557, -0.13140468],\n",
      "        [ 0.991645  ,  0.8672654 ,  1.6072122 , ...,  0.6946604 ,\n",
      "         -0.53743   ,  0.83450216],\n",
      "        ...,\n",
      "        [-0.15279606,  0.09851145,  1.4716264 , ..., -0.21940924,\n",
      "          0.73814   ,  0.19910893],\n",
      "        [ 0.21701768,  0.97522897,  1.3835249 , ...,  0.4479974 ,\n",
      "          0.6017606 , -0.27235514],\n",
      "        [ 0.29221368,  0.1808902 ,  0.74973404, ...,  0.60543257,\n",
      "          1.1432494 ,  0.07314605]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.5132389 ,  0.50970554,  0.19912985, ..., -0.38999215,\n",
      "          0.4052692 , -0.23153389],\n",
      "        [ 0.5394626 , -0.3658086 ,  0.6667344 , ..., -0.39200184,\n",
      "          0.25045055,  0.02019719],\n",
      "        [ 0.7766629 ,  0.6822611 ,  0.7109607 , ..., -0.0420045 ,\n",
      "         -0.37177944,  0.3748228 ],\n",
      "        ...,\n",
      "        [ 0.1622846 ,  0.07057018,  0.6608415 , ..., -0.27227673,\n",
      "          0.36695793, -0.01004261],\n",
      "        [ 0.3120802 ,  0.42583174,  0.5610485 , ..., -0.00480619,\n",
      "          0.34787303, -0.15459618],\n",
      "        [ 0.3906597 ,  0.11558013,  0.36925718, ...,  0.03078479,\n",
      "          0.5419684 , -0.00493214]]], dtype=float32)>), attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\",      \n",
    "                    max_length=MAX_SEQUENCE_LENGTH, # set the length of the sequences\n",
    "    add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False, # not needed for this type of ML task\n",
    "    pad_to_max_length=True, # add 0 pad tokens to the sequences less than max_length\n",
    "                  )\n",
    "inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = model(inputs, output_hidden_states=True)\n",
    "\n",
    "loss = output.loss\n",
    "logits = output.logits\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n",
    "output = tf.keras.layers.Dense(2, activation='softmax')(tf.reshape(output[0], [-1, 1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "980fb59c-5928-4202-b514-e3011a4dda80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.7734848], dtype=float32)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "34e0b800-4cff-4631-89ec-cb6e01053be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.43494776, 0.5650522 ]], dtype=float32)>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eb58e31c-a234-4541-b67e-7ff998204842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 64), dtype=int32, numpy=\n",
      "array([[  101,  8667,   117,  1139,  3676,  1110, 10509,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0]])>, 'attention_mask': <tf.Tensor: shape=(1, 64), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'labels': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]])>}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "945919c0-7cca-40ba-bbe6-8c17fae6a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.29079425], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5006b373-e441-42e3-856d-100a84723ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.4495986 ,  0.09766434, -0.20736787, ...,  0.05780864,\n",
      "          0.04061905, -0.09512451],\n",
      "        [-0.77356714,  0.49493614,  0.40814084, ...,  0.11958291,\n",
      "         -0.31714237,  0.25474828],\n",
      "        [-1.143637  ,  0.41187298, -0.28890803, ...,  0.5126003 ,\n",
      "         -1.2320071 , -0.52256787],\n",
      "        ...,\n",
      "        [-0.14134963, -1.0802779 ,  0.8814317 , ...,  0.5289587 ,\n",
      "         -0.5264092 ,  0.6608591 ],\n",
      "        [ 0.20395406, -1.2452701 ,  0.910847  , ...,  0.40185875,\n",
      "         -0.5353736 ,  0.82868004],\n",
      "        [-0.04057743, -1.1146938 ,  0.71299577, ...,  0.28208214,\n",
      "         -0.648682  ,  0.5204784 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.2717536 , -0.04192287, -0.09590326, ...,  0.04059108,\n",
      "          0.04587968, -0.06677815],\n",
      "        [-1.0057184 ,  0.8226264 ,  0.08506905, ...,  0.0218928 ,\n",
      "         -0.17773648, -0.13762966],\n",
      "        [-0.6432052 ,  0.01664471, -0.18631639, ...,  0.00367083,\n",
      "         -1.47046   , -0.48410627],\n",
      "        ...,\n",
      "        [-0.82927746,  0.28863037,  0.72361857, ..., -0.57800734,\n",
      "          0.1307468 , -0.05393491],\n",
      "        [-0.5045121 ,  0.0227115 ,  0.6665975 , ..., -0.6014166 ,\n",
      "          0.076334  ,  0.18229644],\n",
      "        [-0.78299576,  0.2026111 ,  0.5040815 , ..., -0.7440822 ,\n",
      "          0.07636045,  0.00765086]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.4113537 ,  0.03542183, -0.13542114, ...,  0.02569922,\n",
      "          0.09557049, -0.21680173],\n",
      "        [-1.5705411 ,  0.7848764 ,  0.64523697, ...,  0.05660557,\n",
      "         -0.0944061 ,  0.2905097 ],\n",
      "        [-0.32248846,  0.21803924,  0.01887472, ...,  0.06627898,\n",
      "         -1.6043357 , -0.4186162 ],\n",
      "        ...,\n",
      "        [-0.51147556,  0.32392716, -0.00294674, ..., -0.6548061 ,\n",
      "          0.37816164, -0.38762832],\n",
      "        [-0.17566144,  0.19532612,  0.06606326, ..., -0.74281734,\n",
      "          0.418076  , -0.14147389],\n",
      "        [-0.52168953,  0.14602672, -0.09764053, ..., -0.6127122 ,\n",
      "          0.41315353, -0.17275095]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.747951  ,  0.27842423, -0.02599991, ..., -0.11867957,\n",
      "          0.20770487,  0.05830685],\n",
      "        [-0.864117  ,  0.5627613 ,  0.6533158 , ..., -0.5003373 ,\n",
      "          0.23912533,  0.76711833],\n",
      "        [-0.03846994,  0.35835725,  0.26583165, ..., -0.24068788,\n",
      "         -1.1931301 , -0.07848042],\n",
      "        ...,\n",
      "        [-0.36296296,  0.3771789 ,  0.03935151, ..., -0.5116693 ,\n",
      "          0.27092746, -0.5614311 ],\n",
      "        [-0.21564241,  0.28091723,  0.09653701, ..., -0.43084592,\n",
      "          0.40868828, -0.61329615],\n",
      "        [-0.5051213 ,  0.27645755, -0.00122335, ..., -0.15878648,\n",
      "          0.54011446, -0.3486855 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 1.0046226 ,  0.13807979, -0.19961499, ..., -0.20288283,\n",
      "          0.3202954 , -0.13604575],\n",
      "        [-0.6251106 , -0.23203014,  0.61730826, ..., -0.57891756,\n",
      "          0.3137484 ,  1.0665734 ],\n",
      "        [ 0.09344125,  0.7069855 ,  0.28426725, ...,  0.3439204 ,\n",
      "         -1.5786818 ,  0.1838922 ],\n",
      "        ...,\n",
      "        [-0.42478514, -0.04417733,  0.17523752, ..., -0.57372475,\n",
      "          0.36609566, -0.20999777],\n",
      "        [-0.2135917 ,  0.3937164 ,  0.29970932, ..., -0.2148481 ,\n",
      "          0.46469384, -0.651316  ],\n",
      "        [-0.6423254 ,  0.12481614,  0.04504028, ..., -0.17138043,\n",
      "          0.96727234, -0.04640239]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.65392536,  0.07504879, -0.31819898, ...,  0.13731223,\n",
      "          0.22386964,  0.18493047],\n",
      "        [-0.5588591 , -0.3727954 ,  0.53505117, ..., -0.8424486 ,\n",
      "          0.5102325 ,  0.7580971 ],\n",
      "        [-0.44435364,  0.67128485,  0.17989384, ...,  0.35587835,\n",
      "         -1.310087  ,  0.40854365],\n",
      "        ...,\n",
      "        [-0.726029  ,  0.05631098,  0.4572392 , ..., -0.22703652,\n",
      "          0.48195451, -0.26570398],\n",
      "        [-0.7141562 ,  0.22714898,  0.12022512, ...,  0.16720137,\n",
      "          0.4961822 , -0.5657394 ],\n",
      "        [-0.9107365 ,  0.169726  ,  0.11996478, ...,  0.3050424 ,\n",
      "          0.9599275 , -0.15234566]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.6287231 ,  0.09801637, -0.24016707, ...,  0.08790691,\n",
      "          0.2347407 ,  0.09451795],\n",
      "        [-1.1530353 , -0.86748075,  0.14664774, ...,  0.10651734,\n",
      "          0.24783684,  0.57126683],\n",
      "        [-0.48547286,  0.92328507,  0.06264733, ...,  0.76924384,\n",
      "         -1.3928778 ,  0.8748554 ],\n",
      "        ...,\n",
      "        [-0.6068442 , -0.0624581 ,  0.5005005 , ...,  0.08372362,\n",
      "          0.6212526 ,  0.06608756],\n",
      "        [-0.52677745,  0.21867582,  0.22750771, ...,  0.61828697,\n",
      "          0.42393556,  0.03618112],\n",
      "        [-0.6778434 ,  0.11327402,  0.21741693, ...,  0.65787745,\n",
      "          1.3413128 ,  0.10726638]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.41759467,  0.27723294, -0.43783355, ..., -0.13128594,\n",
      "         -0.16570257, -0.03024263],\n",
      "        [-1.0239758 , -0.9478185 ,  0.04848468, ..., -0.2207675 ,\n",
      "          0.4865424 ,  0.37934306],\n",
      "        [-0.29349983,  1.181611  , -0.0791633 , ...,  0.11689858,\n",
      "         -0.9054741 ,  0.8016735 ],\n",
      "        ...,\n",
      "        [-0.6416546 ,  0.07965674,  0.3929208 , ..., -0.19310921,\n",
      "          0.7803275 ,  0.02712587],\n",
      "        [-0.13811994,  0.6276928 ,  0.05473451, ...,  0.28276286,\n",
      "          0.46254742,  0.11691382],\n",
      "        [-0.25678653,  0.36273727, -0.06770976, ...,  0.29609576,\n",
      "          1.5306716 ,  0.0533267 ]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.14264825,  0.47637373, -0.22101593, ..., -0.42810777,\n",
      "         -0.20844232, -0.11585789],\n",
      "        [-0.63882095, -0.70646167, -0.03192337, ..., -0.66733783,\n",
      "          0.61210364,  0.5088238 ],\n",
      "        [ 0.04127431,  1.4510192 ,  0.0680201 , ...,  0.45462894,\n",
      "         -1.2131885 ,  0.96490586],\n",
      "        ...,\n",
      "        [-0.6114702 ,  0.2410463 ,  0.24431548, ..., -0.20327447,\n",
      "          0.66148573,  0.18824238],\n",
      "        [ 0.1301331 ,  1.1221621 ,  0.12672594, ...,  0.3348631 ,\n",
      "          0.29749873, -0.19243051],\n",
      "        [-0.2000985 ,  0.5785368 , -0.3603623 , ...,  0.20285818,\n",
      "          1.238944  ,  0.33241734]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.3579655 ,  0.7665882 , -0.01365671, ..., -0.56607056,\n",
      "          0.18296602, -0.4623926 ],\n",
      "        [-0.44317377, -0.6184645 ,  0.29880103, ..., -0.3879462 ,\n",
      "          0.50778836,  0.4666363 ],\n",
      "        [ 0.2601076 ,  1.5714998 ,  0.9411452 , ...,  0.63134414,\n",
      "         -1.3295022 ,  1.0811528 ],\n",
      "        ...,\n",
      "        [-0.38738772,  0.43292272,  0.6377402 , ..., -0.2606183 ,\n",
      "          0.49682635, -0.07754751],\n",
      "        [-0.0060748 ,  1.5419927 ,  0.716313  , ...,  0.54585487,\n",
      "          0.26095492, -0.29071754],\n",
      "        [ 0.12226446,  0.75977093,  0.03050062, ...,  0.6707242 ,\n",
      "          0.98377   ,  0.14872758]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.35562766,  1.0946242 , -0.16056128, ..., -0.31538537,\n",
      "          0.6490852 , -0.43823346],\n",
      "        [ 0.36815062, -0.925473  ,  0.5047371 , ..., -0.39709556,\n",
      "          0.28011313,  0.48919666],\n",
      "        [ 1.3953726 ,  1.0439694 ,  1.061358  , ...,  0.8717776 ,\n",
      "         -1.2489814 ,  0.9872713 ],\n",
      "        ...,\n",
      "        [-0.05743501,  0.24588674,  0.7747303 , ..., -0.38251096,\n",
      "          0.46248367,  0.13517934],\n",
      "        [ 0.35850826,  1.3656436 ,  0.5444269 , ...,  0.5006408 ,\n",
      "          0.22670099, -0.19239989],\n",
      "        [ 0.4438493 ,  0.46479774,  0.15534802, ...,  0.8052641 ,\n",
      "          0.8811157 ,  0.31277686]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.4629375 ,  1.1814607 ,  0.27575207, ..., -0.2780588 ,\n",
      "          0.73968107, -0.29886192],\n",
      "        [ 0.3228344 , -0.65143245,  1.0510073 , ..., -0.42321533,\n",
      "          0.24877557, -0.13140468],\n",
      "        [ 0.991645  ,  0.8672654 ,  1.6072122 , ...,  0.6946604 ,\n",
      "         -0.53743   ,  0.83450216],\n",
      "        ...,\n",
      "        [-0.15279606,  0.09851145,  1.4716264 , ..., -0.21940924,\n",
      "          0.73814   ,  0.19910893],\n",
      "        [ 0.21701768,  0.97522897,  1.3835249 , ...,  0.4479974 ,\n",
      "          0.6017606 , -0.27235514],\n",
      "        [ 0.29221368,  0.1808902 ,  0.74973404, ...,  0.60543257,\n",
      "          1.1432494 ,  0.07314605]]], dtype=float32)>, <tf.Tensor: shape=(1, 64, 768), dtype=float32, numpy=\n",
      "array([[[ 0.5132389 ,  0.50970554,  0.19912985, ..., -0.38999215,\n",
      "          0.4052692 , -0.23153389],\n",
      "        [ 0.5394626 , -0.3658086 ,  0.6667344 , ..., -0.39200184,\n",
      "          0.25045055,  0.02019719],\n",
      "        [ 0.7766629 ,  0.6822611 ,  0.7109607 , ..., -0.0420045 ,\n",
      "         -0.37177944,  0.3748228 ],\n",
      "        ...,\n",
      "        [ 0.1622846 ,  0.07057018,  0.6608415 , ..., -0.27227673,\n",
      "          0.36695793, -0.01004261],\n",
      "        [ 0.3120802 ,  0.42583174,  0.5610485 , ..., -0.00480619,\n",
      "          0.34787303, -0.15459618],\n",
      "        [ 0.3906597 ,  0.11558013,  0.36925718, ...,  0.03078479,\n",
      "          0.5419684 , -0.00493214]]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d71cf563-6b0e-4037-9c64-191b6ac0aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d3dfbc9b-d4f7-4a20-aebc-33d0aea227fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.4495986   0.09766434 -0.20736787 ...  0.05780864  0.04061905\n",
      "   -0.09512451]\n",
      "  [-0.77356714  0.49493614  0.40814084 ...  0.11958291 -0.31714237\n",
      "    0.25474828]\n",
      "  [-1.143637    0.41187298 -0.28890803 ...  0.5126003  -1.2320071\n",
      "   -0.52256787]\n",
      "  ...\n",
      "  [-0.14134963 -1.0802779   0.8814317  ...  0.5289587  -0.5264092\n",
      "    0.6608591 ]\n",
      "  [ 0.20395406 -1.2452701   0.910847   ...  0.40185875 -0.5353736\n",
      "    0.82868004]\n",
      "  [-0.04057743 -1.1146938   0.71299577 ...  0.28208214 -0.648682\n",
      "    0.5204784 ]]], shape=(1, 64, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbea578-9f29-4f0e-8fc7-c04c9cfee93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094413c8-76de-4837-8b12-a37c5218e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c030ddd0-2c51-49be-a4de-2d0fd1f1d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello, my dog is cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f011023-9379-4489-9114-8390701c7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76a6d38-4e97-4cc8-bc39-f184454e12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xids = np.zeros((2, SEQ_LEN))\n",
    "Xmask = np.zeros((2, SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab92723a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94259156-5172-4d20-bd9c-6cd885eee302",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\BERT.ipynb Cell 81'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000080?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, sentence \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m([sentence, sentence]):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000080?line=1'>2</a>\u001b[0m     Xids[i, :], Xmask[i, :] \u001b[39m=\u001b[39m tokenize(sentence)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate([sentence, sentence]):\n",
    "    Xids[i, :], Xmask[i, :] = tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3f42cc3e-359a-46f3-ad5a-fe3596a89e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_sent):\n",
    "    tokens = tokenizer.encode_plus(input_sent, max_length=SEQ_LEN,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "575fb059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,  8667.,   117.,  1139.,  3676.,  1110., 10509.,   102.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.],\n",
       "       [  101.,  8667.,   117.,  1139.,  3676.,  1110., 10509.,   102.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a297a63-2232-46d5-98c1-b2d88613627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(50,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, [1, 1]))\n",
    "\n",
    "# restructure dataset format for BERT\n",
    "def map_func(input_ids, masks, labels):\n",
    "    print(input_ids)\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, 1\n",
    "  \n",
    "dataset = dataset.map(map_func)  # apply the mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02cd6a48-47f5-48e0-ac52-a22207d3da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, 'attention_mask': <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37ba4de6-0c38-442e-adae-af080df0f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0024cdb-5a1d-41ce-8dbb-97c30991d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2251: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\",      \n",
    "                    max_length=SEQ_LEN, # set the length of the sequences\n",
    "    add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False, # not needed for this type of ML task\n",
    "    pad_to_max_length=True, # add 0 pad tokens to the sequences less than max_length\n",
    "                  )\n",
    "inputs[\"labels\"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771a9bd1-e8ac-47a8-80bf-fdc35d9a2572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 50), dtype=int32, numpy=\n",
       "array([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(1, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]])>, 'labels': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]])>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867fea6e-d7e4-41d0-9b6f-cfa382d2912a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\BERT.ipynb Cell 89'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=0'>1</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m50\u001b[39m,), name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=1'>2</a>\u001b[0m mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m50\u001b[39m,), name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=3'>4</a>\u001b[0m embeddings \u001b[39m=\u001b[39m bert(input_ids, attention_mask\u001b[39m=\u001b[39mmask)  \u001b[39m# we only keep tensor 0 (last_hidden_state)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=5'>6</a>\u001b[0m \u001b[39m# X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=6'>7</a>\u001b[0m \u001b[39m# X = tf.keras.layers.BatchNormalization()(X)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=7'>8</a>\u001b[0m \u001b[39m# X = tf.keras.layers.Dense(128, activation='relu')(X)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=8'>9</a>\u001b[0m \u001b[39m# X = tf.keras.layers.Dropout(0.1)(X)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=9'>10</a>\u001b[0m \u001b[39m# y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000088?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m[input_ids, mask], outputs\u001b[39m=\u001b[39membeddings)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "# X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "# X = tf.keras.layers.BatchNormalization()(X)\n",
    "# X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "# X = tf.keras.layers.Dropout(0.1)(X)\n",
    "# y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=embeddings)\n",
    "\n",
    "# # freeze the BERT layer\n",
    "# model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8b420955-09e2-4ccf-acb7-2dfdbd311424",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = bert(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b27a8ee9-1a8d-48f9-9d40-20af422d37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 50, 768), dtype=float32, numpy=\n",
       "array([[[ 0.5132389 ,  0.50970554,  0.19912985, ..., -0.38999215,\n",
       "          0.4052692 , -0.23153389],\n",
       "        [ 0.5394626 , -0.3658086 ,  0.6667344 , ..., -0.39200184,\n",
       "          0.25045055,  0.02019719],\n",
       "        [ 0.7766629 ,  0.6822611 ,  0.7109607 , ..., -0.0420045 ,\n",
       "         -0.37177944,  0.3748228 ],\n",
       "        ...,\n",
       "        [ 0.394462  ,  0.13875982,  0.56905514, ...,  0.01595568,\n",
       "          0.51019484, -0.08350345],\n",
       "        [ 0.4279501 ,  0.19366696,  0.20105973, ...,  0.11090432,\n",
       "          0.4984797 , -0.04191069],\n",
       "        [ 0.24493073,  0.2278865 ,  0.6375439 , ..., -0.0600812 ,\n",
       "          0.10338356, -0.02667893]]], dtype=float32)>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4daac380-366c-424f-96b6-4489630435cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = bert(inputs)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f3b44af8-fc3c-4402-8ca5-72faa2cccf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[ 1.29032290e+00,  1.03555655e+00,  7.10960686e-01,\n",
       "         1.37796175e+00,  6.69964492e-01,  2.58656532e-01,\n",
       "         6.63879871e-01,  4.89272952e-01,  4.74286199e-01,\n",
       "         2.62343884e-01,  1.63592830e-01,  9.18961346e-01,\n",
       "         4.71095532e-01,  9.40465391e-01,  4.23889682e-02,\n",
       "         4.41716045e-01,  3.66609573e-01,  5.24260283e-01,\n",
       "         5.53627014e-01,  1.08773804e+00,  1.00412657e-02,\n",
       "        -1.65543094e-01,  6.14793241e-01,  8.58429447e-02,\n",
       "         2.58111030e-01,  5.22225238e-02,  1.06781805e+00,\n",
       "         1.90751648e+00,  1.78271145e-01,  8.79378259e-01,\n",
       "         4.80118811e-01,  7.05948472e-01,  9.09945011e-01,\n",
       "         8.20003569e-01,  2.32826993e-02,  5.24063110e-01,\n",
       "         5.66205025e-01,  8.27183545e-01,  1.25916183e-01,\n",
       "         6.86729014e-01,  4.44131285e-01,  2.33704790e-01,\n",
       "         4.91281956e-01,  1.56880721e-01,  8.58769596e-01,\n",
       "        -1.53291151e-02,  1.20558053e-01,  3.13506842e-01,\n",
       "         1.03474736e-01,  7.26718664e-01,  3.92366886e-01,\n",
       "         3.69427145e-01,  1.39055178e-01,  1.04017329e+00,\n",
       "         5.13360858e-01,  1.35757625e-01, -2.62305178e-02,\n",
       "         3.18176121e-01,  1.67328790e-01,  7.08757758e-01,\n",
       "         1.49893433e-01,  5.76207042e-01,  6.56254470e-01,\n",
       "         3.13892722e-01,  7.09360838e-03,  5.49461126e-01,\n",
       "         4.45585191e-01,  5.94579458e-01,  4.66655642e-01,\n",
       "         3.44206274e-01,  8.83071601e-01,  5.43720722e-01,\n",
       "         3.51670057e-01,  1.10387266e+00,  5.29266059e-01,\n",
       "         5.60977280e-01,  6.35247409e-01,  4.05545175e-01,\n",
       "         4.27225828e-01,  6.04157031e-01,  3.40357602e-01,\n",
       "         1.01040888e+00,  5.78519940e-01,  3.40966582e-01,\n",
       "         2.54469573e-01,  1.76026201e+00,  3.80703092e-01,\n",
       "         5.91061890e-01,  4.92492348e-01,  5.45233369e-01,\n",
       "         6.03368521e-01,  2.09110051e-01,  3.57103199e-01,\n",
       "         1.49758011e-01,  6.51867568e-01,  7.13848591e-01,\n",
       "         3.44272584e-01,  1.80321395e-01,  6.95753050e+00,\n",
       "         4.05485868e-01,  2.50066400e-01,  7.20799804e-01,\n",
       "         1.01934969e+00,  1.03024065e+00,  2.38681525e-01,\n",
       "         4.59672511e-01,  4.40457165e-01,  1.27481848e-01,\n",
       "         6.46917999e-01,  1.28881609e+00,  1.02831447e+00,\n",
       "         7.54093289e-01,  6.20005131e-01,  3.13134134e-01,\n",
       "         3.81090552e-01,  5.60322940e-01,  1.06404626e+00,\n",
       "         7.96324134e-01,  8.08478236e-01,  2.23039433e-01,\n",
       "         8.93014312e-01,  4.02148873e-01,  1.39692020e+00,\n",
       "         5.03622413e-01,  5.90551555e-01,  6.29018188e-01,\n",
       "         3.54403943e-01,  9.69730243e-02,  1.00498235e+00,\n",
       "         6.14572644e-01,  3.06092054e-02,  6.35198593e-01,\n",
       "         4.01905745e-01,  8.90061498e-01,  4.47172463e-01,\n",
       "         7.18156040e-01,  5.84281445e-01,  4.59647983e-01,\n",
       "         2.27550954e-01,  4.43974942e-01,  6.39569104e-01,\n",
       "         6.27475418e-03,  1.15529394e+00,  5.37145257e-01,\n",
       "         5.24065077e-01,  3.11807060e+00,  3.39464813e-01,\n",
       "         2.42376357e-01,  2.57223487e-01,  4.19156998e-01,\n",
       "        -4.25907448e-02, -9.21768993e-02,  9.00356472e-02,\n",
       "         2.83820927e-01,  8.13899994e-01,  4.25732911e-01,\n",
       "         5.49702287e-01,  8.11733902e-01,  6.23705447e-01,\n",
       "         2.34005883e-01,  1.35367960e-02,  1.66238129e-01,\n",
       "         6.14166141e-01,  6.07904434e-01,  6.07893348e-01,\n",
       "         4.03810441e-01,  6.19039357e-01,  4.22910810e-01,\n",
       "         2.47011855e-01,  1.33910728e+00,  3.42820436e-01,\n",
       "         8.78556192e-01,  1.97100803e-01,  7.06273556e-01,\n",
       "         7.33419001e-01,  8.07789683e-01,  5.44055998e-01,\n",
       "         3.56641263e-01,  4.52059269e-01,  4.06951755e-01,\n",
       "         4.39392179e-01,  4.31049705e-01,  4.87908781e-01,\n",
       "         5.14175594e-01,  5.71401477e-01,  4.87697303e-01,\n",
       "         7.31511235e-01,  1.57040656e+00,  2.10782456e+00,\n",
       "         4.01654840e-01,  9.17849302e-01, -1.72668219e-01,\n",
       "         1.99948192e-01,  8.36576104e-01,  4.86933827e-01,\n",
       "         6.23444021e-02,  5.08389533e-01,  4.34600472e-01,\n",
       "         8.39332163e-01,  6.17751062e-01,  1.75517127e-01,\n",
       "         2.42504984e-01,  1.31322324e-01,  8.17026556e-01,\n",
       "         7.11403549e-01,  5.01850657e-02,  5.93748212e-01,\n",
       "         5.26884615e-01, -1.14803344e-01,  1.65052295e-01,\n",
       "         2.08687231e-01,  5.01020610e-01,  1.31579387e+00,\n",
       "         7.42160499e-01,  5.81744909e-01,  7.87328035e-02,\n",
       "         2.31262654e-01,  1.37763453e+00,  7.37109363e-01,\n",
       "         1.05173063e+00,  3.09170961e-01,  3.41730803e-01,\n",
       "         3.43953103e-01,  6.99749172e-01,  4.92162526e-01,\n",
       "         3.65246564e-01,  1.11133444e+00,  1.07564986e-01,\n",
       "         9.43869054e-01,  4.78530705e-01,  3.87638211e-01,\n",
       "         4.01331365e-01,  9.78107989e-01,  1.18945166e-01,\n",
       "         1.51710853e-01,  3.71927500e-01,  3.25202823e-01,\n",
       "         5.33327341e-01,  1.63254648e-01,  5.97464561e-01,\n",
       "         2.15761513e-01,  1.23369358e-01,  1.28661430e+00,\n",
       "         3.65555525e-01,  9.36438963e-02,  6.51955843e-01,\n",
       "         7.18085289e-01,  5.93985260e-01,  6.07308745e-01,\n",
       "         1.37328613e+00,  1.15474808e+00,  8.39357972e-01,\n",
       "        -2.14607462e-01,  5.73246121e-01,  1.29327607e+00,\n",
       "         3.79471958e-01,  5.71790636e-01,  6.14015758e-01,\n",
       "         3.68094370e-02,  1.76439196e-01,  6.15465164e-01,\n",
       "         1.11009490e+00,  3.95213842e-01,  4.98857349e-01,\n",
       "         3.84455115e-01,  4.82842296e-01,  4.10558850e-01,\n",
       "         5.26535332e-01, -3.55579555e-01,  1.10095811e+00,\n",
       "         4.96304393e-01,  3.07486534e-01,  5.13526917e-01,\n",
       "         1.07562447e+00,  5.49615324e-01,  4.60793108e-01,\n",
       "         5.40693402e-01,  2.42804274e-01,  4.44117785e-02,\n",
       "         9.40169580e-03,  2.57917017e-01,  8.39342177e-01,\n",
       "         5.19302070e-01,  1.51307547e+00, -3.49079818e-02,\n",
       "         8.21642160e-01,  6.86385810e-01,  5.82597971e-01,\n",
       "         3.18001598e-01,  4.57742691e+00,  6.96869254e-01,\n",
       "         2.05850333e-01,  4.41217750e-01,  3.22578669e-01,\n",
       "         2.99941897e-01,  2.49921128e-01,  2.29208842e-01,\n",
       "         7.34305561e-01,  3.66754264e-01,  9.90160704e-01,\n",
       "         1.24432182e+00,  9.54881430e-01,  2.37960875e-01,\n",
       "         3.17021132e-01, -2.10059091e-01,  5.91398656e-01,\n",
       "         5.00375152e-01,  1.68058252e+00,  5.19544423e-01,\n",
       "         7.75273621e-01,  1.73802090e+00,  7.94797480e-01,\n",
       "         4.12338465e-01,  7.00729907e-01,  1.78911313e-01,\n",
       "        -1.90066159e-01,  6.43641114e-01,  7.71207929e-01,\n",
       "         1.61627904e-01,  2.63703048e-01,  8.06757212e-01,\n",
       "         1.03330731e+00,  7.17480540e-01,  5.11684775e-01,\n",
       "         5.34399331e-01,  4.52066332e-01,  5.91003776e-01,\n",
       "         6.44530892e-01,  4.02180105e-01, -6.86481595e-03,\n",
       "         4.69873250e-01,  3.67794186e-01,  1.67367235e-01,\n",
       "         1.48416147e-01,  1.84396505e-01,  4.40753490e-01,\n",
       "         6.33072779e-02,  4.21158373e-01,  6.62520111e-01,\n",
       "         3.77256989e-01,  4.29008543e-01,  4.81674075e-01,\n",
       "         8.81814539e-01,  4.96576309e-01,  5.51615298e-01,\n",
       "         4.09079432e-01,  4.69172448e-01,  5.51506639e-01,\n",
       "         2.85875708e-01,  3.81378829e-01,  1.46331966e-01,\n",
       "         4.88055408e-01,  4.17661011e-01,  3.88921052e-01,\n",
       "         4.74059314e-01,  5.70273995e-01,  1.98259488e-01,\n",
       "         9.26163197e-01,  2.84768522e-01,  1.74016964e+00,\n",
       "         6.80382907e-01, -1.17458075e-01,  1.02508664e+00,\n",
       "         7.63552308e-01,  8.40151310e-01,  5.82505584e-01,\n",
       "         5.57874799e-01,  7.65297651e-01,  2.89391011e-01,\n",
       "         1.26633263e+00,  1.42669880e+00,  8.50808918e-01,\n",
       "         5.12639642e-01,  3.41493011e-01,  6.44488454e-01,\n",
       "         6.18155956e-01,  2.98744082e-01,  6.82235360e-01,\n",
       "         2.95636058e-01,  1.03198767e-01,  5.25075719e-02,\n",
       "         5.70428014e-01,  8.41660976e-01,  2.62588054e-01,\n",
       "         5.40629923e-01,  5.90413928e-01,  5.73896885e-01,\n",
       "         3.69066268e-01,  1.42810416e+00,  7.06032515e-01,\n",
       "         5.96030712e-01,  2.24221781e-01,  7.02142239e-01,\n",
       "         6.01983070e-01,  2.29490340e-01,  5.40864050e-01,\n",
       "         1.64276630e-01,  7.41661131e-01,  1.08727694e+00,\n",
       "         5.81332088e-01,  9.83359292e-04,  1.14468895e-01,\n",
       "         6.31760895e-01,  7.21530437e-01,  2.04712758e-03,\n",
       "         4.15106058e-01,  9.08574760e-02,  1.86012030e-01,\n",
       "         3.32714170e-01,  5.92658937e-01,  4.63455200e-01,\n",
       "         3.33820194e-01,  4.61918473e-01,  9.88283873e-01,\n",
       "         2.79729426e-01,  1.82958454e-01,  5.22950649e-01,\n",
       "         6.25438929e-01,  7.33775437e-01,  6.28644228e-01,\n",
       "         4.70862031e-01,  1.57640189e-01,  1.91386789e-01,\n",
       "         3.91657591e-01,  1.29363000e+00,  3.84949923e-01,\n",
       "         2.97764987e-01,  1.72481552e-01,  2.94089764e-01,\n",
       "         7.42838740e-01,  2.88586855e-01,  4.48505670e-01,\n",
       "         9.71656218e-02,  3.66980970e-01,  4.04987574e-01,\n",
       "         3.74046504e-01,  4.71409559e-01,  5.41188478e-01,\n",
       "         4.52023178e-01,  5.64719820e+00,  3.68263513e-01,\n",
       "         2.32782751e-01,  7.34958589e-01,  3.90876196e-02,\n",
       "         2.22986668e-01,  3.73392612e-01,  5.77238619e-01,\n",
       "         7.91674733e-01,  9.53860164e-01,  5.32239795e-01,\n",
       "         7.04058409e-01,  3.52770388e-01,  5.07544637e-01,\n",
       "         1.92410454e-01,  6.74586833e-01,  3.86059225e-01,\n",
       "         1.01365256e+00,  3.96354496e-01,  3.63063306e-01,\n",
       "         1.55511308e+00,  3.31029028e-01,  6.67635679e-01,\n",
       "         2.53358752e-01,  1.30133569e+00,  5.05341911e+00,\n",
       "         2.69565493e-01,  6.04574680e-01,  1.52829576e+00,\n",
       "         7.88910091e-02,  7.50193536e-01,  7.04605758e-01,\n",
       "         4.36518133e-01,  1.77591789e+00,  1.14432609e+00,\n",
       "         2.91036218e-01,  3.59693825e-01,  6.87487006e-01,\n",
       "         7.23599195e-01,  3.79467428e-01,  2.86647350e-01,\n",
       "         9.65544343e-01,  1.39611685e+00, -5.26212938e-02,\n",
       "         7.05286741e-01,  6.63677156e-01,  2.99880624e-01,\n",
       "         1.30911303e+00,  3.79807204e-01,  3.55909765e-01,\n",
       "         1.13616240e+00,  5.57444334e-01,  6.61492825e-01,\n",
       "         7.60737598e-01,  5.00739157e-01,  6.45539165e-01,\n",
       "         6.02965355e-01,  3.83224428e-01,  9.18232501e-01,\n",
       "         2.02998921e-01,  4.22533482e-01,  2.25755945e-03,\n",
       "         4.96380121e-01,  2.53217161e-01,  3.80122870e-01,\n",
       "         2.22491920e-01,  5.34387112e-01,  1.56561732e-01,\n",
       "         3.12100053e-01,  7.95973599e-01,  6.38437271e-01,\n",
       "         6.15494728e-01, -3.53183925e-01,  4.13716853e-01,\n",
       "         8.65837097e-01,  9.83260691e-01,  2.00985819e-01,\n",
       "         9.99247789e-01,  4.58852977e-01,  9.86280680e-01,\n",
       "         5.39427996e-01,  4.24749792e-01,  1.57483444e-01,\n",
       "         2.82324642e-01,  3.47495019e-01,  3.27171266e-01,\n",
       "         9.75733638e-01,  1.20682448e-01,  5.83817005e-01,\n",
       "         4.07229364e-01,  4.32852864e-01, -1.11530222e-01,\n",
       "         4.05532002e-01,  3.65541577e-01,  6.70810044e-01,\n",
       "         9.18686032e-01,  4.88115162e-01,  5.39297462e-01,\n",
       "         3.49104047e-01,  3.21395844e-01,  7.43041337e-01,\n",
       "         4.30252314e-01,  1.42566884e+00,  5.02667487e-01,\n",
       "         1.54668927e-01,  1.31126016e-01,  1.27323675e+00,\n",
       "         2.95570910e-01,  1.08614147e+00,  3.78876656e-01,\n",
       "         5.09998202e-01,  3.72840881e-01,  6.58922911e-01,\n",
       "        -4.88613434e-02,  5.89190543e-01,  1.67193115e+00,\n",
       "         2.62021601e-01,  5.52746594e-01,  1.61207628e+00,\n",
       "         8.33197951e-01,  5.37018895e-01,  5.31105042e-01,\n",
       "         4.50768083e-01,  4.42482680e-01,  8.31742823e-01,\n",
       "         3.13314974e-01,  7.87270427e-01,  5.92693746e-01,\n",
       "         2.52207071e-01,  5.26704788e-01,  2.50100225e-01,\n",
       "        -1.76982917e-02,  3.23077321e-01,  4.67791885e-01,\n",
       "         6.27066791e-01,  6.55351162e-01,  3.74789387e-01,\n",
       "         1.20083022e+00,  8.69683802e-01,  6.51795447e-01,\n",
       "         1.15369582e+00,  4.54475641e-01,  7.55257845e-01,\n",
       "         7.36170769e-01,  2.52701133e-01,  3.42154294e-01,\n",
       "         6.32930696e-01,  1.92038462e-01,  3.28905553e-01,\n",
       "         6.74933314e-01,  4.56880987e-01,  3.60451967e-01,\n",
       "         5.06942451e-01,  2.28446141e-01,  5.04315436e-01,\n",
       "         1.78388402e-01,  2.95783639e-01, -1.73835143e-01,\n",
       "         2.46530727e-01,  7.76467323e-01,  7.01219261e-01,\n",
       "         1.62158334e+00,  7.49140859e-01,  8.01996171e-01,\n",
       "         4.79171157e-01,  5.32198489e-01,  2.56220043e-01,\n",
       "         1.13136232e+00,  4.24615526e+00,  4.37304556e-01,\n",
       "         7.48679996e-01,  3.09550345e-01,  2.44164869e-01,\n",
       "         9.27627444e-01,  4.19796258e-01,  2.88388759e-01,\n",
       "         7.05124259e-01,  6.20542288e-01,  4.45029855e-01,\n",
       "         6.69413984e-01,  8.34601164e-01,  2.57262826e-01,\n",
       "         7.57145658e-02,  4.12572831e-01,  5.11538804e-01,\n",
       "         8.26558888e-01,  5.98018765e-01,  7.39384651e-01,\n",
       "         2.08107635e-01,  1.99970245e-01,  2.58583605e-01,\n",
       "         3.78347754e-01,  3.14989597e-01,  5.05051732e-01,\n",
       "         6.02576315e-01,  4.44150835e-01,  5.87228000e-01,\n",
       "         7.39087582e-01,  2.49501809e-01,  1.17763329e+00,\n",
       "         1.35629606e+00,  2.79949099e-01,  3.70594412e-01,\n",
       "         2.94681340e-01,  2.51531422e-01,  6.70020819e-01,\n",
       "         4.42235649e-01,  3.88180822e-01,  5.74664772e-01,\n",
       "         9.43053886e-02,  1.70667723e-01,  3.15233022e-01,\n",
       "         1.36038944e-01,  5.82568944e-01,  3.71579856e-01,\n",
       "         6.92590714e-01,  7.61028707e-01,  2.18454123e-01,\n",
       "         2.40360469e-01,  2.13531435e-01,  1.01004028e+00,\n",
       "        -1.61512539e-01,  5.39233625e-01,  2.61618644e-01,\n",
       "         2.68710017e+00,  2.02371955e-01,  2.45641395e-01,\n",
       "         9.66162860e-01,  3.63491684e-01,  5.30871153e-01,\n",
       "         3.25428575e-01,  5.97867310e-01,  1.75694585e+00,\n",
       "         5.08144796e-01,  8.61769140e-01,  3.85460854e-01,\n",
       "         6.39600277e-01,  3.42103243e-01,  8.60570133e-01,\n",
       "         8.00513625e-01,  3.89658898e-01,  4.34234113e-01,\n",
       "         3.99062067e-01,  4.19649482e-01,  4.23636287e-01,\n",
       "         4.13056761e-01,  6.83702588e-01,  2.60891080e-01,\n",
       "         1.04637313e+00,  4.52358961e-01,  1.28012046e-01,\n",
       "         8.17453206e-01,  8.02322507e-01,  2.72906482e-01,\n",
       "         5.91363490e-01,  3.43587697e-01,  8.27310383e-02,\n",
       "         1.24237931e+00,  4.66714464e-02,  5.73599160e-01,\n",
       "         1.08830929e+00,  5.97819328e-01,  7.89352179e-01,\n",
       "         7.49518871e-02,  4.32834804e-01, -9.03204083e-03,\n",
       "         1.46688625e-01,  1.00872982e+00,  1.66765943e-01,\n",
       "         1.44515181e+00,  3.41973901e-01, -5.03277071e-02,\n",
       "         5.48650861e-01,  4.14391011e-01,  7.63888776e-01,\n",
       "         4.83680189e-01,  3.67238164e-01,  6.09319627e-01,\n",
       "         6.11383766e-02,  1.51867405e-01,  5.34215748e-01,\n",
       "         2.76586205e-01,  7.07606792e-01,  2.11252153e-01,\n",
       "         1.01579614e-01,  6.34434104e-01,  5.00844717e-01,\n",
       "         6.40153050e-01,  4.42382336e-01,  8.16101909e-01,\n",
       "         5.78205645e-01,  1.50849009e+00,  1.33326612e-02,\n",
       "         1.89241946e-01,  1.02405763e+00,  5.23956358e-01,\n",
       "         5.08856058e-01,  8.39209408e-02,  7.88163483e-01,\n",
       "         1.07466984e+00,  1.25317797e-01,  5.61578631e-01,\n",
       "         4.80799884e-01,  3.26107085e-01,  7.87371278e-01,\n",
       "         1.10876727e+00,  2.71733791e-01,  2.57798338e+00,\n",
       "         1.74929455e-01,  2.11386457e-01,  3.10566455e-01,\n",
       "         7.91178763e-01,  1.00300804e-01,  4.40001011e-01,\n",
       "         3.44070137e-01,  6.07872546e-01,  5.39340436e-01,\n",
       "         1.70186007e+00,  5.03832579e-01,  6.14152133e-01,\n",
       "         5.34716010e-01,  1.46760255e-01,  6.31557107e-01,\n",
       "         4.82155889e-01,  5.47453649e-02,  7.86030650e-01,\n",
       "         4.73545164e-01,  1.19726300e+00,  5.42133510e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "643d1909-8d26-4a48-89b3-8e822ce6f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "embeddings1 = bert(inputs)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "print(type(embeddings1))\n",
    "\n",
    "X1 = tf.keras.layers.GlobalMaxPool1D()(embeddings1)  # reduce tensor dimensionality\n",
    "X1 = tf.keras.layers.BatchNormalization()(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bebfc71b-b806-4a7e-8dfb-37139f4d94bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[ 1.28967798e+00,  1.03503895e+00,  7.10605383e-01,\n",
       "         1.37727308e+00,  6.69629633e-01,  2.58527249e-01,\n",
       "         6.63548052e-01,  4.89028424e-01,  4.74049151e-01,\n",
       "         2.62212753e-01,  1.63511068e-01,  9.18502092e-01,\n",
       "         4.70860094e-01,  9.39995348e-01,  4.23677824e-02,\n",
       "         4.41495270e-01,  3.66426349e-01,  5.23998260e-01,\n",
       "         5.53350329e-01,  1.08719444e+00,  1.00362469e-02,\n",
       "        -1.65460363e-01,  6.14485979e-01,  8.58000442e-02,\n",
       "         2.57982016e-01,  5.21964245e-02,  1.06728435e+00,\n",
       "         1.90656316e+00,  1.78182051e-01,  8.78938735e-01,\n",
       "         4.79878843e-01,  7.05595672e-01,  9.09490228e-01,\n",
       "         8.19593728e-01,  2.32710633e-02,  5.23801208e-01,\n",
       "         5.65922022e-01,  8.26770127e-01,  1.25853255e-01,\n",
       "         6.86385810e-01,  4.43909317e-01,  2.33587995e-01,\n",
       "         4.91036415e-01,  1.56802312e-01,  8.58340383e-01,\n",
       "        -1.53214540e-02,  1.20497800e-01,  3.13350141e-01,\n",
       "         1.03423022e-01,  7.26355433e-01,  3.92170787e-01,\n",
       "         3.69242519e-01,  1.38985679e-01,  1.03965342e+00,\n",
       "         5.13104260e-01,  1.35689780e-01, -2.62174085e-02,\n",
       "         3.18017095e-01,  1.67245165e-01,  7.08403528e-01,\n",
       "         1.49818525e-01,  5.75919092e-01,  6.55926466e-01,\n",
       "         3.13735843e-01,  7.09006330e-03,  5.49186528e-01,\n",
       "         4.45362508e-01,  5.94282269e-01,  4.66422409e-01,\n",
       "         3.44034255e-01,  8.82630229e-01,  5.43448985e-01,\n",
       "         3.51494282e-01,  1.10332096e+00,  5.29001534e-01,\n",
       "         5.60696900e-01,  6.34929895e-01,  4.05342489e-01,\n",
       "         4.27012295e-01,  6.03855073e-01,  3.40187490e-01,\n",
       "         1.00990391e+00,  5.78230798e-01,  3.40796173e-01,\n",
       "         2.54342407e-01,  1.75938225e+00,  3.80512834e-01,\n",
       "         5.90766490e-01,  4.92246211e-01,  5.44960856e-01,\n",
       "         6.03066981e-01,  2.09005535e-01,  3.56924713e-01,\n",
       "         1.49683163e-01,  6.51541770e-01,  7.13491797e-01,\n",
       "         3.44100535e-01,  1.80231273e-01,  6.95405340e+00,\n",
       "         4.05283213e-01,  2.49941424e-01,  7.20439553e-01,\n",
       "         1.01884019e+00,  1.02972579e+00,  2.38562241e-01,\n",
       "         4.59442765e-01,  4.40237045e-01,  1.27418131e-01,\n",
       "         6.46594703e-01,  1.28817201e+00,  1.02780056e+00,\n",
       "         7.53716409e-01,  6.19695246e-01,  3.12977642e-01,\n",
       "         3.80900085e-01,  5.60042918e-01,  1.06351447e+00,\n",
       "         7.95926154e-01,  8.08074176e-01,  2.22927958e-01,\n",
       "         8.92567992e-01,  4.01947886e-01,  1.39622200e+00,\n",
       "         5.03370702e-01,  5.90256393e-01,  6.28703833e-01,\n",
       "         3.54226828e-01,  9.69245583e-02,  1.00448012e+00,\n",
       "         6.14265501e-01,  3.05939075e-02,  6.34881139e-01,\n",
       "         4.01704878e-01,  8.89616668e-01,  4.46948975e-01,\n",
       "         7.17797101e-01,  5.83989441e-01,  4.59418267e-01,\n",
       "         2.27437228e-01,  4.43753064e-01,  6.39249444e-01,\n",
       "         6.27161795e-03,  1.15471649e+00,  5.36876798e-01,\n",
       "         5.23803174e-01,  3.11651230e+00,  3.39295149e-01,\n",
       "         2.42255226e-01,  2.57094920e-01,  4.18947518e-01,\n",
       "        -4.25694585e-02, -9.21308324e-02,  8.99906456e-02,\n",
       "         2.83679068e-01,  8.13493192e-01,  4.25520122e-01,\n",
       "         5.49427569e-01,  8.11328232e-01,  6.23393714e-01,\n",
       "         2.33888924e-01,  1.35300308e-02,  1.66155040e-01,\n",
       "         6.13859177e-01,  6.07600629e-01,  6.07589543e-01,\n",
       "         4.03608620e-01,  6.18729949e-01,  4.22699451e-01,\n",
       "         2.46888399e-01,  1.33843803e+00,  3.42649102e-01,\n",
       "         8.78117085e-01,  1.97002292e-01,  7.05920577e-01,\n",
       "         7.33052433e-01,  8.07385981e-01,  5.43784082e-01,\n",
       "         3.56463015e-01,  4.51833338e-01,  4.06748354e-01,\n",
       "         4.39172566e-01,  4.30834264e-01,  4.87664938e-01,\n",
       "         5.13918638e-01,  5.71115911e-01,  4.87453550e-01,\n",
       "         7.31145620e-01,  1.56962168e+00,  2.10677099e+00,\n",
       "         4.01454091e-01,  9.17390585e-01, -1.72581926e-01,\n",
       "         1.99848264e-01,  8.36157978e-01,  4.86690462e-01,\n",
       "         6.23132437e-02,  5.08135438e-01,  4.34383273e-01,\n",
       "         8.38912666e-01,  6.17442310e-01,  1.75429404e-01,\n",
       "         2.42383778e-01,  1.31256685e-01,  8.16618204e-01,\n",
       "         7.11048007e-01,  5.01599833e-02,  5.93451440e-01,\n",
       "         5.26621282e-01, -1.14745967e-01,  1.64969802e-01,\n",
       "         2.08582938e-01,  5.00770211e-01,  1.31513631e+00,\n",
       "         7.41789579e-01,  5.81454158e-01,  7.86934569e-02,\n",
       "         2.31147066e-01,  1.37694597e+00,  7.36740947e-01,\n",
       "         1.05120504e+00,  3.09016436e-01,  3.41560006e-01,\n",
       "         3.43781203e-01,  6.99399471e-01,  4.91916537e-01,\n",
       "         3.65064025e-01,  1.11077905e+00,  1.07511230e-01,\n",
       "         9.43397343e-01,  4.78291541e-01,  3.87444466e-01,\n",
       "         4.01130795e-01,  9.77619171e-01,  1.18885718e-01,\n",
       "         1.51635036e-01,  3.71741623e-01,  3.25040281e-01,\n",
       "         5.33060789e-01,  1.63173050e-01,  5.97165942e-01,\n",
       "         2.15653673e-01,  1.23307697e-01,  1.28597128e+00,\n",
       "         3.65372837e-01,  9.35970917e-02,  6.51629984e-01,\n",
       "         7.17726409e-01,  5.93688369e-01,  6.07005239e-01,\n",
       "         1.37259972e+00,  1.15417099e+00,  8.38938475e-01,\n",
       "        -2.14500204e-01,  5.72959602e-01,  1.29262972e+00,\n",
       "         3.79282296e-01,  5.71504891e-01,  6.13708854e-01,\n",
       "         3.67910415e-02,  1.76351011e-01,  6.15157545e-01,\n",
       "         1.10954010e+00,  3.95016313e-01,  4.98608023e-01,\n",
       "         3.84262979e-01,  4.82600987e-01,  4.10353661e-01,\n",
       "         5.26272178e-01, -3.55401844e-01,  1.10040784e+00,\n",
       "         4.96056348e-01,  3.07332844e-01,  5.13270259e-01,\n",
       "         1.07508683e+00,  5.49340606e-01,  4.60562795e-01,\n",
       "         5.40423155e-01,  2.42682919e-01,  4.43895832e-02,\n",
       "         9.39699728e-03,  2.57788122e-01,  8.38922679e-01,\n",
       "         5.19042552e-01,  1.51231921e+00, -3.48905362e-02,\n",
       "         8.21231544e-01,  6.86042786e-01,  5.82306802e-01,\n",
       "         3.17842662e-01,  4.57513905e+00,  6.96520984e-01,\n",
       "         2.05747455e-01,  4.40997243e-01,  3.22417438e-01,\n",
       "         2.99791992e-01,  2.49796227e-01,  2.29094282e-01,\n",
       "         7.33938575e-01,  3.66570979e-01,  9.89665866e-01,\n",
       "         1.24369991e+00,  9.54404175e-01,  2.37841949e-01,\n",
       "         3.16862702e-01, -2.09954113e-01,  5.91103077e-01,\n",
       "         5.00125051e-01,  1.67974257e+00,  5.19284785e-01,\n",
       "         7.74886131e-01,  1.73715222e+00,  7.94400275e-01,\n",
       "         4.12132382e-01,  7.00379670e-01,  1.78821892e-01,\n",
       "        -1.89971164e-01,  6.43319428e-01,  7.70822465e-01,\n",
       "         1.61547124e-01,  2.63571262e-01,  8.06353986e-01,\n",
       "         1.03279090e+00,  7.17121959e-01,  5.11429071e-01,\n",
       "         5.34132242e-01,  4.51840401e-01,  5.90708375e-01,\n",
       "         6.44208789e-01,  4.01979089e-01, -6.86138496e-03,\n",
       "         4.69638407e-01,  3.67610365e-01,  1.67283595e-01,\n",
       "         1.48341969e-01,  1.84304342e-01,  4.40533221e-01,\n",
       "         6.32756352e-02,  4.20947880e-01,  6.62189007e-01,\n",
       "         3.77068430e-01,  4.28794146e-01,  4.81433332e-01,\n",
       "         8.81373823e-01,  4.96328115e-01,  5.51339626e-01,\n",
       "         4.08874989e-01,  4.68937963e-01,  5.51231027e-01,\n",
       "         2.85732836e-01,  3.81188214e-01,  1.46258831e-01,\n",
       "         4.87811476e-01,  4.17452276e-01,  3.88726681e-01,\n",
       "         4.73822385e-01,  5.69988966e-01,  1.98160395e-01,\n",
       "         9.25700307e-01,  2.84626186e-01,  1.73929989e+00,\n",
       "         6.80042863e-01, -1.17399372e-01,  1.02457428e+00,\n",
       "         7.63170719e-01,  8.39731395e-01,  5.82214475e-01,\n",
       "         5.57595968e-01,  7.64915168e-01,  2.89246380e-01,\n",
       "         1.26569974e+00,  1.42598581e+00,  8.50383699e-01,\n",
       "         5.12383461e-01,  3.41322333e-01,  6.44166350e-01,\n",
       "         6.17847025e-01,  2.98594773e-01,  6.81894362e-01,\n",
       "         2.95488298e-01,  1.03147186e-01,  5.24813309e-02,\n",
       "         5.70142925e-01,  8.41240346e-01,  2.62456805e-01,\n",
       "         5.40359735e-01,  5.90118825e-01,  5.73610067e-01,\n",
       "         3.68881822e-01,  1.42739046e+00,  7.05679655e-01,\n",
       "         5.95732808e-01,  2.24109724e-01,  7.01791346e-01,\n",
       "         6.01682186e-01,  2.29375646e-01,  5.40593743e-01,\n",
       "         1.64194524e-01,  7.41290450e-01,  1.08673358e+00,\n",
       "         5.81041574e-01,  9.82867787e-04,  1.14411682e-01,\n",
       "         6.31445169e-01,  7.21169829e-01,  2.04610452e-03,\n",
       "         4.14898604e-01,  9.08120647e-02,  1.85919061e-01,\n",
       "         3.32547873e-01,  5.92362761e-01,  4.63223577e-01,\n",
       "         3.33653361e-01,  4.61687624e-01,  9.87789929e-01,\n",
       "         2.79589623e-01,  1.82867020e-01,  5.22689283e-01,\n",
       "         6.25126362e-01,  7.33408689e-01,  6.28330052e-01,\n",
       "         4.70626712e-01,  1.57561406e-01,  1.91291139e-01,\n",
       "         3.91461849e-01,  1.29298341e+00,  3.84757519e-01,\n",
       "         2.97616154e-01,  1.72395349e-01,  2.93942779e-01,\n",
       "         7.42467463e-01,  2.88442612e-01,  4.48281527e-01,\n",
       "         9.71170589e-02,  3.66797566e-01,  4.04785156e-01,\n",
       "         3.73859555e-01,  4.71173942e-01,  5.40917993e-01,\n",
       "         4.51797277e-01,  5.64437580e+00,  3.68079454e-01,\n",
       "         2.32666403e-01,  7.34591246e-01,  3.90680842e-02,\n",
       "         2.22875223e-01,  3.73205990e-01,  5.76950133e-01,\n",
       "         7.91279078e-01,  9.53383446e-01,  5.31973779e-01,\n",
       "         7.03706503e-01,  3.52594078e-01,  5.07290959e-01,\n",
       "         1.92314297e-01,  6.74249709e-01,  3.85866284e-01,\n",
       "         1.01314592e+00,  3.96156400e-01,  3.62881839e-01,\n",
       "         1.55433583e+00,  3.30863595e-01,  6.67302012e-01,\n",
       "         2.53232121e-01,  1.30068529e+00,  5.05089331e+00,\n",
       "         2.69430757e-01,  6.04272544e-01,  1.52753198e+00,\n",
       "         7.88515806e-02,  7.49818623e-01,  7.04253614e-01,\n",
       "         4.36299980e-01,  1.77503026e+00,  1.14375412e+00,\n",
       "         2.90890753e-01,  3.59514058e-01,  6.87143385e-01,\n",
       "         7.23237574e-01,  3.79277766e-01,  2.86504090e-01,\n",
       "         9.65061784e-01,  1.39541912e+00, -5.25949933e-02,\n",
       "         7.04934239e-01,  6.63345456e-01,  2.99730748e-01,\n",
       "         1.30845881e+00,  3.79617393e-01,  3.55731875e-01,\n",
       "         1.13559461e+00,  5.57165742e-01,  6.61162198e-01,\n",
       "         7.60357380e-01,  5.00488877e-01,  6.45216525e-01,\n",
       "         6.02663994e-01,  3.83032888e-01,  9.17773604e-01,\n",
       "         2.02897459e-01,  4.22322303e-01,  2.25643115e-03,\n",
       "         4.96132046e-01,  2.53090620e-01,  3.79932880e-01,\n",
       "         2.22380728e-01,  5.34120023e-01,  1.56483486e-01,\n",
       "         3.11944067e-01,  7.95575798e-01,  6.38118207e-01,\n",
       "         6.15187109e-01, -3.53007406e-01,  4.13510084e-01,\n",
       "         8.65404367e-01,  9.82769251e-01,  2.00885370e-01,\n",
       "         9.98748362e-01,  4.58623648e-01,  9.85787749e-01,\n",
       "         5.39158404e-01,  4.24537510e-01,  1.57404736e-01,\n",
       "         2.82183528e-01,  3.47321361e-01,  3.27007741e-01,\n",
       "         9.75245953e-01,  1.20622136e-01,  5.83525240e-01,\n",
       "         4.07025844e-01,  4.32636529e-01, -1.11474484e-01,\n",
       "         4.05329317e-01,  3.65358889e-01,  6.70474768e-01,\n",
       "         9.18226898e-01,  4.87871200e-01,  5.39027929e-01,\n",
       "         3.48929584e-01,  3.21235210e-01,  7.42670000e-01,\n",
       "         4.30037290e-01,  1.42495632e+00,  5.02416253e-01,\n",
       "         1.54591620e-01,  1.31060481e-01,  1.27260041e+00,\n",
       "         2.95423180e-01,  1.08559859e+00,  3.78687292e-01,\n",
       "         5.09743333e-01,  3.72654527e-01,  6.58593595e-01,\n",
       "        -4.88369241e-02,  5.88896096e-01,  1.67109549e+00,\n",
       "         2.61890650e-01,  5.52470326e-01,  1.61127055e+00,\n",
       "         8.32781553e-01,  5.36750495e-01,  5.30839622e-01,\n",
       "         4.50542808e-01,  4.42261547e-01,  8.31327140e-01,\n",
       "         3.13158393e-01,  7.86876976e-01,  5.92397511e-01,\n",
       "         2.52081007e-01,  5.26441574e-01,  2.49975234e-01,\n",
       "        -1.76894460e-02,  3.22915852e-01,  4.67558086e-01,\n",
       "         6.26753390e-01,  6.55023634e-01,  3.74602079e-01,\n",
       "         1.20023012e+00,  8.69249165e-01,  6.51469707e-01,\n",
       "         1.15311921e+00,  4.54248488e-01,  7.54880369e-01,\n",
       "         7.35802829e-01,  2.52574831e-01,  3.41983289e-01,\n",
       "         6.32614374e-01,  1.91942483e-01,  3.28741163e-01,\n",
       "         6.74596012e-01,  4.56652641e-01,  3.60271811e-01,\n",
       "         5.06689072e-01,  2.28331968e-01,  5.04063368e-01,\n",
       "         1.78299248e-01,  2.95635819e-01, -1.73748270e-01,\n",
       "         2.46407509e-01,  7.76079237e-01,  7.00868785e-01,\n",
       "         1.62077296e+00,  7.48766422e-01,  8.01595330e-01,\n",
       "         4.78931665e-01,  5.31932533e-01,  2.56091982e-01,\n",
       "         1.13079691e+00,  4.24403286e+00,  4.37085986e-01,\n",
       "         7.48305798e-01,  3.09395641e-01,  2.44042844e-01,\n",
       "         9.27163839e-01,  4.19586450e-01,  2.88244635e-01,\n",
       "         7.04771876e-01,  6.20232165e-01,  4.44807440e-01,\n",
       "         6.69079423e-01,  8.34184051e-01,  2.57134259e-01,\n",
       "         7.56767243e-02,  4.12366629e-01,  5.11283159e-01,\n",
       "         8.26145768e-01,  5.97719908e-01,  7.39015102e-01,\n",
       "         2.08003625e-01,  1.99870303e-01,  2.58454382e-01,\n",
       "         3.78158659e-01,  3.14832181e-01,  5.04799306e-01,\n",
       "         6.02275133e-01,  4.43928868e-01,  5.86934507e-01,\n",
       "         7.38718212e-01,  2.49377117e-01,  1.17704475e+00,\n",
       "         1.35561824e+00,  2.79809177e-01,  3.70409191e-01,\n",
       "         2.94534057e-01,  2.51405716e-01,  6.69685960e-01,\n",
       "         4.42014635e-01,  3.87986809e-01,  5.74377537e-01,\n",
       "         9.42582563e-02,  1.70582429e-01,  3.15075487e-01,\n",
       "         1.35970950e-01,  5.82277775e-01,  3.71394157e-01,\n",
       "         6.92244589e-01,  7.60648370e-01,  2.18344942e-01,\n",
       "         2.40240335e-01,  2.13424712e-01,  1.00953543e+00,\n",
       "        -1.61431819e-01,  5.38964152e-01,  2.61487901e-01,\n",
       "         2.68575716e+00,  2.02270806e-01,  2.45518625e-01,\n",
       "         9.65680003e-01,  3.63310009e-01,  5.30605853e-01,\n",
       "         3.25265944e-01,  5.97568512e-01,  1.75606775e+00,\n",
       "         5.07890821e-01,  8.61338437e-01,  3.85268211e-01,\n",
       "         6.39280617e-01,  3.41932267e-01,  8.60140026e-01,\n",
       "         8.00113559e-01,  3.89464140e-01,  4.34017092e-01,\n",
       "         3.98862630e-01,  4.19439733e-01,  4.23424572e-01,\n",
       "         4.12850320e-01,  6.83360875e-01,  2.60760695e-01,\n",
       "         1.04585016e+00,  4.52132881e-01,  1.27948061e-01,\n",
       "         8.17044675e-01,  8.01921546e-01,  2.72770077e-01,\n",
       "         5.91067910e-01,  3.43415976e-01,  8.26896876e-02,\n",
       "         1.24175835e+00,  4.66481224e-02,  5.73312461e-01,\n",
       "         1.08776534e+00,  5.97520530e-01,  7.88957655e-01,\n",
       "         7.49144256e-02,  4.32618469e-01, -9.02752671e-03,\n",
       "         1.46615312e-01,  1.00822568e+00,  1.66682601e-01,\n",
       "         1.44442952e+00,  3.41802984e-01, -5.03025539e-02,\n",
       "         5.48376679e-01,  4.14183915e-01,  7.63507009e-01,\n",
       "         4.83438462e-01,  3.67054611e-01,  6.09015107e-01,\n",
       "         6.11078218e-02,  1.51791498e-01,  5.33948779e-01,\n",
       "         2.76447982e-01,  7.07253158e-01,  2.11146578e-01,\n",
       "         1.01528846e-01,  6.34117007e-01,  5.00594378e-01,\n",
       "         6.39833093e-01,  4.42161232e-01,  8.15694034e-01,\n",
       "         5.77916682e-01,  1.50773621e+00,  1.33259976e-02,\n",
       "         1.89147368e-01,  1.02354586e+00,  5.23694515e-01,\n",
       "         5.08601725e-01,  8.38790014e-02,  7.87769556e-01,\n",
       "         1.07413268e+00,  1.25255167e-01,  5.61297953e-01,\n",
       "         4.80559587e-01,  3.25944096e-01,  7.86977768e-01,\n",
       "         1.10821307e+00,  2.71597981e-01,  2.57669497e+00,\n",
       "         1.74842030e-01,  2.11280808e-01,  3.10411245e-01,\n",
       "         7.90783346e-01,  1.00250676e-01,  4.39781100e-01,\n",
       "         3.43898177e-01,  6.07568741e-01,  5.39070904e-01,\n",
       "         1.70100951e+00,  5.03580749e-01,  6.13845170e-01,\n",
       "         5.34448743e-01,  1.46686912e-01,  6.31241441e-01,\n",
       "         4.81914908e-01,  5.47180027e-02,  7.85637796e-01,\n",
       "         4.73308504e-01,  1.19666457e+00,  5.41862547e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "69eff69a-8371-46d7-a19f-40264d2c190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = bert(input_ids, attention_mask=mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3d3ee1ff-3df2-423d-afb9-21a99d033542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.keras_tensor.KerasTensor"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3809beb3-cd94-44a9-9d6a-59a05585e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(50, ), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50, ), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state)\n",
    "\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(X)  # adjust based on number of sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# # freeze the BERT layer\n",
    "# model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9d91acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_7 (Global  (None, 768)         0           ['tf_bert_model_1[7][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_7[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          98432       ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)           (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            129         ['dropout_80[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,411,905\n",
      "Trainable params: 100,097\n",
      "Non-trainable params: 108,311,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca05874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_7 (Global  (None, 768)         0           ['tf_bert_model_1[7][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 768)         3072        ['global_max_pooling1d_7[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          98432       ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)           (None, 128)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            129         ['dropout_80[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,411,905\n",
      "Trainable params: 100,097\n",
      "Non-trainable params: 108,311,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6b983d1-9dfa-40f7-9b2d-6691e184bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a05677e-4486-4be5-8eac-97fcc01a89e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc49ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "369aea79-2ac8-4b9f-bc53-b8debebe4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(metrics=[acc])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7193c0d0-0dae-4155-b601-49314e8399b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b18f1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(1, 50), dtype=float64, numpy=\n",
      "array([[  101.,  8667.,   117.,  1139.,  3676.,  1110., 10509.,   102.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "            0.,     0.]])>, 'attention_mask': <tf.Tensor: shape=(1, 50), dtype=float64, numpy=\n",
      "array([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]])>}, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.batch(1):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f67ade1-507b-4fde-aab8-6b470961c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.int32, name='input_ids'), name='input_ids', description=\"created by layer 'input_ids'\"), but it was called on an input with incompatible shape (50,).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.int32, name='attention_mask'), name='attention_mask', description=\"created by layer 'attention_mask'\"), but it was called on an input with incompatible shape (50,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1127, in call  *\n            outputs = self.bert(\n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 773, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received:\n           input_ids=tf.Tensor(shape=(50,), dtype=int32)\n           attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n           token_type_ids=None\n           position_ids=None\n           head_mask=None\n           inputs_embeds=None\n           encoder_hidden_states=None\n           encoder_attention_mask=None\n           past_key_values=None\n           use_cache=True\n           output_attentions=False\n           output_hidden_states=False\n           return_dict=True\n           training=True\n           kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received:\n       input_ids=tf.Tensor(shape=(50,), dtype=int32)\n       attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n       token_type_ids=None\n       position_ids=None\n       head_mask=None\n       inputs_embeds=None\n       encoder_hidden_states=None\n       encoder_attention_mask=None\n       past_key_values=None\n       use_cache=None\n       output_attentions=None\n       output_hidden_states=None\n       return_dict=None\n       training=True\n       kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\BERT.ipynb Cell 105'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000100?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1129'>1130</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1130'>1131</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1127, in call  *\n            outputs = self.bert(\n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 773, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received:\n           input_ids=tf.Tensor(shape=(50,), dtype=int32)\n           attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n           token_type_ids=None\n           position_ids=None\n           head_mask=None\n           inputs_embeds=None\n           encoder_hidden_states=None\n           encoder_attention_mask=None\n           past_key_values=None\n           use_cache=True\n           output_attentions=False\n           output_hidden_states=False\n           return_dict=True\n           training=True\n           kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received:\n       input_ids=tf.Tensor(shape=(50,), dtype=int32)\n       attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n       token_type_ids=None\n       position_ids=None\n       head_mask=None\n       inputs_embeds=None\n       encoder_hidden_states=None\n       encoder_attention_mask=None\n       past_key_values=None\n       use_cache=None\n       output_attentions=None\n       output_hidden_states=None\n       return_dict=None\n       training=True\n       kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6923d3b2-604f-491b-a1f1-62dc899763dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6928972005844116, 0.6918980479240417], 'accuracy': [0.5, 1.0]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27e31581-567a-4ffe-8ea5-9724f258a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.int32, name='input_ids'), name='input_ids', description=\"created by layer 'input_ids'\"), but it was called on an input with incompatible shape (50,).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.int32, name='attention_mask'), name='attention_mask', description=\"created by layer 'attention_mask'\"), but it was called on an input with incompatible shape (50,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1127, in call  *\n            outputs = self.bert(\n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 773, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received:\n           input_ids=tf.Tensor(shape=(50,), dtype=int32)\n           attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n           token_type_ids=None\n           position_ids=None\n           head_mask=None\n           inputs_embeds=None\n           encoder_hidden_states=None\n           encoder_attention_mask=None\n           past_key_values=None\n           use_cache=True\n           output_attentions=False\n           output_hidden_states=False\n           return_dict=True\n           training=False\n           kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received:\n       input_ids=tf.Tensor(shape=(50,), dtype=int32)\n       attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n       token_type_ids=None\n       position_ids=None\n       head_mask=None\n       inputs_embeds=None\n       encoder_hidden_states=None\n       encoder_attention_mask=None\n       past_key_values=None\n       use_cache=None\n       output_attentions=None\n       output_hidden_states=None\n       return_dict=None\n       training=False\n       kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\BERT.ipynb Cell 107'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000102?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(dataset)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1129'>1130</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1130'>1131</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n    \n    in user code:\n    \n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1127, in call  *\n            outputs = self.bert(\n        File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 773, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received:\n           input_ids=tf.Tensor(shape=(50,), dtype=int32)\n           attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n           token_type_ids=None\n           position_ids=None\n           head_mask=None\n           inputs_embeds=None\n           encoder_hidden_states=None\n           encoder_attention_mask=None\n           past_key_values=None\n           use_cache=True\n           output_attentions=False\n           output_hidden_states=False\n           return_dict=True\n           training=False\n           kwargs=<class 'inspect._empty'>\n    \n    \n    Call arguments received:\n       input_ids=tf.Tensor(shape=(50,), dtype=int32)\n       attention_mask=tf.Tensor(shape=(50,), dtype=int32)\n       token_type_ids=None\n       position_ids=None\n       head_mask=None\n       inputs_embeds=None\n       encoder_hidden_states=None\n       encoder_attention_mask=None\n       past_key_values=None\n       use_cache=None\n       output_attentions=None\n       output_hidden_states=None\n       return_dict=None\n       training=False\n       kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    "test = model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1581ca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57419837],\n",
       "       [0.57419837]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "436d22ea-329d-4fa6-a482-1284752c4058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 768)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "020516a9-500b-4f1f-853e-485c452676f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 0\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede282f0-cf6b-4106-9dc2-33a8b06e48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\losses.py\", line 1664, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 50, 768) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\jupyters\\BERT.ipynb Cell 109'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000105?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mloss, metrics\u001b[39m=\u001b[39m[acc])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000105?line=7'>8</a>\u001b[0m \u001b[39m# and train it\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vojta/Desktop/diploma/jupyters/BERT.ipynb#ch0000105?line=8'>9</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49mbatch(\u001b[39m1\u001b[39;49m), epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1126'>1127</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1127'>1128</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1128'>1129</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1129'>1130</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vojta/Desktop/diploma/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1130'>1131</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\losses.py\", line 1664, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\Vojta\\Desktop\\diploma\\venv\\lib\\site-packages\\keras\\backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 50, 768) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "# and train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5229bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 31s 352ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 0.6919 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 1s 264ms/step - loss: 0.6909 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 0.6899 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 225ms/step - loss: 0.6889 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.6879 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.6869 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.6859 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 231ms/step - loss: 0.6849 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 1s 311ms/step - loss: 0.6839 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 268ms/step - loss: 0.6830 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 236ms/step - loss: 0.6820 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 0.6810 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 1s 252ms/step - loss: 0.6800 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.6790 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.6780 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 1s 256ms/step - loss: 0.6771 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.6761 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.6751 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 220ms/step - loss: 0.6741 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.batch(1), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275574ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
